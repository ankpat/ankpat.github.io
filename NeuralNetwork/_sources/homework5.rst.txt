
Homework 5
==========

.. contents:: Table of Contents
    :depth: 2

For this assignment, create a neural network program using a language of your choice that
implements the feedforward, back propagation algorithm for a network with the following topology
like the one in the lecture:

.. graphviz::

    digraph simple_network {
        rankdir=LR;

        node [shape=plaintext,width=0.25,label=<x<SUB>1</SUB>>]; x_1;
        node [shape=plaintext,width=0.25,label=<x<SUB>2</SUB>>]; x_2;
        node [shape=square,width=.25,fillcolor=black,style=filled,label=""]; input_0, input_1;
        node [shape=circle,width=.5,fillcolor=white,label=""]; h_0_0, h_0_1;
        node [shape=circle,width=.5,fillcolor=white,label=""]; o_0;
        node [shape=circle,width=.5,style=invisible,label=""]; output;


        x_1 -> input_0;
        x_2 -> input_1;
        input_0 -> h_0_0;
        input_1 -> h_0_0;

        input_0 -> h_0_1;
        input_1 -> h_0_1;

        h_0_0 -> o_0;
        h_0_1 -> o_0;

        o_0 -> output [label="Output"];
    }




Testing
-------


Use the example from class to verify that the network performs correctly.

The initial weights for each node of the hidden layer are 0.3 for all inputs and the initial
weights for all inputs to the output layer are 0.8.

The Gain factor :math:`\eta = 1.0` and all biases are set to 0.

The Input {1, 2} should generate a desired output of 0.7.

Biases will not be updated for this run of the training algorithm.



Initial Network
~~~~~~~~~~~~~~~

.. graphviz::

    digraph network{
    rankdir=LR;
    node [shape=plaintext,width=0.25,label=<x<SUB>1</SUB>>]; x_1;
    node [shape=plaintext,width=0.25,label=<x<SUB>2</SUB>>]; x_2;
    node [shape=square,width=.25,fillcolor=black,style=filled,label=""]; input_0, input_1
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; h_0_0;
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; h_0_1;
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; o_0;
    node [shape=circle,width=.5,style=invisible,label=""]; output;
    
    
    x_1 -> input_0;
    x_2 -> input_1;
    input_0 -> h_0_0 [label="0.300000"];
    input_1 -> h_0_0 [label="0.300000"];
    input_0 -> h_0_1 [label="0.300000"];
    input_1 -> h_0_1 [label="0.300000"];
    h_0_0 -> o_0 [label="0.800000"];
    h_0_1 -> o_0 [label="0.800000"];
    o_0 -> output [label="Output"];
    }
    

Iteration 1
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 2] -> [0.7109495026250039, 0.7109495026250039] -> [0.757223870792493]

Error:  :math:`e = 0.700000 - 0.757224 = -0.0572238708`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0016372857`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7572238708)(1 - 0.2427761292)(-0.0572238708) = -0.0105198007`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7925209530 = 0.8000000000 + 1.000000(-0.0105198007)(0.7109495026)`

:math:`w_2 = 0.7925209530 = 0.8000000000 + 1.000000(-0.0105198007)(0.7109495026)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0017294578 = (0.7109495026)(0.2890504974)((-0.010520)(0.800000))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.2982705422 = 0.3000000000 + 1.000000(-0.0017294578)(1.0000000000)`

:math:`w_2 = 0.2965410844 = 0.3000000000 + 1.000000(-0.0017294578)(2.0000000000)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0017294578 = (0.7109495026)(0.2890504974)((-0.010520)(0.800000))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.2982705422 = 0.3000000000 + 1.000000(-0.0017294578)(1.0000000000)`

:math:`w_2 = 0.2965410844 = 0.3000000000 + 1.000000(-0.0017294578)(2.0000000000)`

.. graphviz::

    digraph network{
    rankdir=LR;
    node [shape=plaintext,width=0.25,label=<x<SUB>1</SUB>>]; x_1;
    node [shape=plaintext,width=0.25,label=<x<SUB>2</SUB>>]; x_2;
    node [shape=square,width=.25,fillcolor=black,style=filled,label=""]; input_0, input_1
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; h_0_0;
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; h_0_1;
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; o_0;
    node [shape=circle,width=.5,style=invisible,label=""]; output;
    
    
    x_1 -> input_0;
    x_2 -> input_1;
    input_0 -> h_0_0 [label="0.298271"];
    input_1 -> h_0_0 [label="0.296541"];
    input_0 -> h_0_1 [label="0.298271"];
    input_1 -> h_0_1 [label="0.296541"];
    h_0_0 -> o_0 [label="0.792521"];
    h_0_1 -> o_0 [label="0.792521"];
    o_0 -> output [label="Output"];
    }
    


Validation Test
~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

[1, 2] -> [0.7091692457152926, 0.7091692457152926] -> [0.7547415782405359]

Error:  :math:`e = 0.700000 - 0.754742 = -0.0547415782`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0014983202`

0.7547415782405359

Alternating Training Method
---------------------------


For each cycle of this training procedure, present the first input/output pair,
perform the back propagation technique to update the weights, then present the second
input/output pair and again perform the back propagation technique to update the
weights. This constitutes a single cycle.

These are the results of executing 15 iterations of training with this method with the following
inputs:

.. math:: ( 1, 1) \rightarrow 0.9

.. math:: (-1, -1) \rightarrow 0.05


.. contents:: Iterations
    :depth: 2
    :local:


Iteration 1
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6456563062257954, 0.6456563062257954] -> [0.7375067919437424]

Error:  :math:`e = 0.900000 - 0.737507 = 0.1624932081`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0132020213`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7375067919)(1 - 0.2624932081)(0.1624932081) = 0.0314571453`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8203105042 = 0.8000000000 + 1.000000(0.0314571453)(0.6456563062)`

:math:`w_2 = 0.8203105042 = 0.8000000000 + 1.000000(0.0314571453)(0.6456563062)`

:math:`bias = 0.0314571453 = 0.0000000000 + (1.0000000000)(0.0314571453)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0057575193 = (0.6456563062)(0.3543436938)((0.031457)(0.800000))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3057575193 = 0.3000000000 + 1.000000(0.0057575193)(1.0000000000)`

:math:`w_2 = 0.3057575193 = 0.3000000000 + 1.000000(0.0057575193)(1.0000000000)`

:math:`bias = 0.0057575193 = 0.0000000000 + (1.0000000000)(0.0057575193)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0057575193 = (0.6456563062)(0.3543436938)((0.031457)(0.800000))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3057575193 = 0.3000000000 + 1.000000(0.0057575193)(1.0000000000)`

:math:`w_2 = 0.3057575193 = 0.3000000000 + 1.000000(0.0057575193)(1.0000000000)`

:math:`bias = 0.0057575193 = 0.0000000000 + (1.0000000000)(0.0057575193)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.3530275714624256, 0.3530275714624256] -> [0.6480871448043597]

Error:  :math:`e = 0.050000 - 0.648087 = -0.5980871448`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1788541164`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6480871448)(1 - 0.3519128552)(-0.5980871448) = -0.1364058533`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7721554771 = 0.8203105042 + 1.000000(-0.1364058533)(0.3530275715)`

:math:`w_2 = 0.7721554771 = 0.8203105042 + 1.000000(-0.1364058533)(0.3530275715)`

:math:`bias = -0.1049487080 = 0.0314571453 + (1.0000000000)(-0.1364058533)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0255567531 = (0.3530275715)(0.6469724285)((-0.136406)(0.820311))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3313142724 = 0.3057575193 + 1.000000(-0.0255567531)(-1.0000000000)`

:math:`w_2 = 0.3313142724 = 0.3057575193 + 1.000000(-0.0255567531)(-1.0000000000)`

:math:`bias = -0.0197992338 = 0.0057575193 + (1.0000000000)(-0.0255567531)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0255567531 = (0.3530275715)(0.6469724285)((-0.136406)(0.820311))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3313142724 = 0.3057575193 + 1.000000(-0.0255567531)(-1.0000000000)`

:math:`w_2 = 0.3313142724 = 0.3057575193 + 1.000000(-0.0255567531)(-1.0000000000)`

:math:`bias = -0.0197992338 = 0.0057575193 + (1.0000000000)(-0.0255567531)`

Iteration 2
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.655392749869442, 0.655392749869442] -> [0.7124230633601731]

Error:  :math:`e = 0.900000 - 0.712423 = 0.1875769366`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0175925536`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7124230634)(1 - 0.2875769366)(0.1875769366) = 0.0384300954`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7973422830 = 0.7721554771 + 1.000000(0.0384300954)(0.6553927499)`

:math:`w_2 = 0.7973422830 = 0.7721554771 + 1.000000(0.0384300954)(0.6553927499)`

:math:`bias = -0.0665186126 = -0.1049487080 + (1.0000000000)(0.0384300954)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0067019666 = (0.6553927499)(0.3446072501)((0.038430)(0.772155))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3380162390 = 0.3313142724 + 1.000000(0.0067019666)(1.0000000000)`

:math:`w_2 = 0.3380162390 = 0.3313142724 + 1.000000(0.0067019666)(1.0000000000)`

:math:`bias = -0.0130972672 = -0.0197992338 + (1.0000000000)(0.0067019666)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0067019666 = (0.6553927499)(0.3446072501)((0.038430)(0.772155))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3380162390 = 0.3313142724 + 1.000000(0.0067019666)(1.0000000000)`

:math:`w_2 = 0.3380162390 = 0.3313142724 + 1.000000(0.0067019666)(1.0000000000)`

:math:`bias = -0.0130972672 = -0.0197992338 + (1.0000000000)(0.0067019666)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.3342266937006828, 0.3342266937006828] -> [0.6145473298197918]

Error:  :math:`e = 0.050000 - 0.614547 = -0.5645473298`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1593568438`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6145473298)(1 - 0.3854526702)(-0.5645473298) = -0.1337293557`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7526463626 = 0.7973422830 + 1.000000(-0.1337293557)(0.3342266937)`

:math:`w_2 = 0.7526463626 = 0.7973422830 + 1.000000(-0.1337293557)(0.3342266937)`

:math:`bias = -0.2002479683 = -0.0665186126 + (1.0000000000)(-0.1337293557)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0237267939 = (0.3342266937)(0.6657733063)((-0.133729)(0.797342))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3617430330 = 0.3380162390 + 1.000000(-0.0237267939)(-1.0000000000)`

:math:`w_2 = 0.3617430330 = 0.3380162390 + 1.000000(-0.0237267939)(-1.0000000000)`

:math:`bias = -0.0368240612 = -0.0130972672 + (1.0000000000)(-0.0237267939)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0237267939 = (0.3342266937)(0.6657733063)((-0.133729)(0.797342))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3617430330 = 0.3380162390 + 1.000000(-0.0237267939)(-1.0000000000)`

:math:`w_2 = 0.3617430330 = 0.3380162390 + 1.000000(-0.0237267939)(-1.0000000000)`

:math:`bias = -0.0368240612 = -0.0130972672 + (1.0000000000)(-0.0237267939)`

Iteration 3
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6652239621897171, 0.6652239621897171] -> [0.6902116190615286]

Error:  :math:`e = 0.900000 - 0.690212 = 0.2097883809`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0220055824`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6902116191)(1 - 0.3097883809)(0.2097883809) = 0.0448568551`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7824862175 = 0.7526463626 + 1.000000(0.0448568551)(0.6652239622)`

:math:`w_2 = 0.7824862175 = 0.7526463626 + 1.000000(0.0448568551)(0.6652239622)`

:math:`bias = -0.1553911132 = -0.2002479683 + (1.0000000000)(0.0448568551)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0075186876 = (0.6652239622)(0.3347760378)((0.044857)(0.752646))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3692617206 = 0.3617430330 + 1.000000(0.0075186876)(1.0000000000)`

:math:`w_2 = 0.3692617206 = 0.3617430330 + 1.000000(0.0075186876)(1.0000000000)`

:math:`bias = -0.0293053736 = -0.0368240612 + (1.0000000000)(0.0075186876)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0075186876 = (0.6652239622)(0.3347760378)((0.044857)(0.752646))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3692617206 = 0.3617430330 + 1.000000(0.0075186876)(1.0000000000)`

:math:`w_2 = 0.3692617206 = 0.3617430330 + 1.000000(0.0075186876)(1.0000000000)`

:math:`bias = -0.0293053736 = -0.0368240612 + (1.0000000000)(0.0075186876)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.3169489642824979, 0.3169489642824979] -> [0.5843424027299755]

Error:  :math:`e = 0.050000 - 0.584342 = -0.5343424027`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1427609017`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.5843424027)(1 - 0.4156575973)(-0.5343424027) = -0.1297844807`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7413511607 = 0.7824862175 + 1.000000(-0.1297844807)(0.3169489643)`

:math:`w_2 = 0.7413511607 = 0.7824862175 + 1.000000(-0.1297844807)(0.3169489643)`

:math:`bias = -0.2851755939 = -0.1553911132 + (1.0000000000)(-0.1297844807)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0219857837 = (0.3169489643)(0.6830510357)((-0.129784)(0.782486))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3912475043 = 0.3692617206 + 1.000000(-0.0219857837)(-1.0000000000)`

:math:`w_2 = 0.3912475043 = 0.3692617206 + 1.000000(-0.0219857837)(-1.0000000000)`

:math:`bias = -0.0512911573 = -0.0293053736 + (1.0000000000)(-0.0219857837)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0219857837 = (0.3169489643)(0.6830510357)((-0.129784)(0.782486))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3912475043 = 0.3692617206 + 1.000000(-0.0219857837)(-1.0000000000)`

:math:`w_2 = 0.3912475043 = 0.3692617206 + 1.000000(-0.0219857837)(-1.0000000000)`

:math:`bias = -0.0512911573 = -0.0293053736 + (1.0000000000)(-0.0219857837)`

Iteration 4
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6750693938417854, 0.6750693938417854] -> [0.6716707531521338]

Error:  :math:`e = 0.900000 - 0.671671 = 0.2283292468`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0260671225`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6716707532)(1 - 0.3283292468)(0.2283292468) = 0.0503532553`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7753431023 = 0.7413511607 + 1.000000(0.0503532553)(0.6750693938)`

:math:`w_2 = 0.7753431023 = 0.7413511607 + 1.000000(0.0503532553)(0.6750693938)`

:math:`bias = -0.2348223386 = -0.2851755939 + (1.0000000000)(0.0503532553)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0081882400 = (0.6750693938)(0.3249306062)((0.050353)(0.741351))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3994357443 = 0.3912475043 + 1.000000(0.0081882400)(1.0000000000)`

:math:`w_2 = 0.3994357443 = 0.3912475043 + 1.000000(0.0081882400)(1.0000000000)`

:math:`bias = -0.0431029173 = -0.0512911573 + (1.0000000000)(0.0081882400)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0081882400 = (0.6750693938)(0.3249306062)((0.050353)(0.741351))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3994357443 = 0.3912475043 + 1.000000(0.0081882400)(1.0000000000)`

:math:`w_2 = 0.3994357443 = 0.3912475043 + 1.000000(0.0081882400)(1.0000000000)`

:math:`bias = -0.0431029173 = -0.0512911573 + (1.0000000000)(0.0081882400)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.301119114313073, 0.301119114313073] -> [0.5577705761028883]

Error:  :math:`e = 0.050000 - 0.557771 = -0.5077705761`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1289154790`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.5577705761)(1 - 0.4422294239)(-0.5077705761) = -0.1252479905`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7376285383 = 0.7753431023 + 1.000000(-0.1252479905)(0.3011191143)`

:math:`w_2 = 0.7376285383 = 0.7753431023 + 1.000000(-0.1252479905)(0.3011191143)`

:math:`bias = -0.3600703291 = -0.2348223386 + (1.0000000000)(-0.1252479905)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0204364841 = (0.3011191143)(0.6988808857)((-0.125248)(0.775343))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4198722284 = 0.3994357443 + 1.000000(-0.0204364841)(-1.0000000000)`

:math:`w_2 = 0.4198722284 = 0.3994357443 + 1.000000(-0.0204364841)(-1.0000000000)`

:math:`bias = -0.0635394014 = -0.0431029173 + (1.0000000000)(-0.0204364841)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0204364841 = (0.3011191143)(0.6988808857)((-0.125248)(0.775343))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4198722284 = 0.3994357443 + 1.000000(-0.0204364841)(-1.0000000000)`

:math:`w_2 = 0.4198722284 = 0.3994357443 + 1.000000(-0.0204364841)(-1.0000000000)`

:math:`bias = -0.0635394014 = -0.0431029173 + (1.0000000000)(-0.0204364841)`

Iteration 5
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6848616407292034, 0.6848616407292034] -> [0.6570728030723777]

Error:  :math:`e = 0.900000 - 0.657073 = 0.2429271969`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0295068115`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6570728031)(1 - 0.3429271969)(0.2429271969) = 0.0547383321`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7751167223 = 0.7376285383 + 1.000000(0.0547383321)(0.6848616407)`

:math:`w_2 = 0.7751167223 = 0.7376285383 + 1.000000(0.0547383321)(0.6848616407)`

:math:`bias = -0.3053319970 = -0.3600703291 + (1.0000000000)(0.0547383321)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0087143176 = (0.6848616407)(0.3151383593)((0.054738)(0.737629))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4285865459 = 0.4198722284 + 1.000000(0.0087143176)(1.0000000000)`

:math:`w_2 = 0.4285865459 = 0.4198722284 + 1.000000(0.0087143176)(1.0000000000)`

:math:`bias = -0.0548250838 = -0.0635394014 + (1.0000000000)(0.0087143176)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0087143176 = (0.6848616407)(0.3151383593)((0.054738)(0.737629))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4285865459 = 0.4198722284 + 1.000000(0.0087143176)(1.0000000000)`

:math:`w_2 = 0.4285865459 = 0.4198722284 + 1.000000(0.0087143176)(1.0000000000)`

:math:`bias = -0.0548250838 = -0.0635394014 + (1.0000000000)(0.0087143176)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.28659112278496096, 0.28659112278496096] -> [0.5346820029005475]

Error:  :math:`e = 0.050000 - 0.534682 = -0.4846820029`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1174583220`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.5346820029)(1 - 0.4653179971)(-0.4846820029) = -0.1205875052`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7405574138 = 0.7751167223 + 1.000000(-0.1205875052)(0.2865911228)`

:math:`w_2 = 0.7405574138 = 0.7751167223 + 1.000000(-0.1205875052)(0.2865911228)`

:math:`bias = -0.4259195021 = -0.3053319970 + (1.0000000000)(-0.1205875052)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0191104388 = (0.2865911228)(0.7134088772)((-0.120588)(0.775117))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4476969848 = 0.4285865459 + 1.000000(-0.0191104388)(-1.0000000000)`

:math:`w_2 = 0.4476969848 = 0.4285865459 + 1.000000(-0.0191104388)(-1.0000000000)`

:math:`bias = -0.0739355226 = -0.0548250838 + (1.0000000000)(-0.0191104388)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0191104388 = (0.2865911228)(0.7134088772)((-0.120588)(0.775117))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4476969848 = 0.4285865459 + 1.000000(-0.0191104388)(-1.0000000000)`

:math:`w_2 = 0.4476969848 = 0.4285865459 + 1.000000(-0.0191104388)(-1.0000000000)`

:math:`bias = -0.0739355226 = -0.0548250838 + (1.0000000000)(-0.0191104388)`

Iteration 6
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.694545840171616, 0.694545840171616] -> [0.64629267209827]

Error:  :math:`e = 0.900000 - 0.646293 = 0.2537073279`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0321837041`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6462926721)(1 - 0.3537073279)(0.2537073279) = 0.0579971029`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7808390604 = 0.7405574138 + 1.000000(0.0579971029)(0.6945458402)`

:math:`w_2 = 0.7808390604 = 0.7405574138 + 1.000000(0.0579971029)(0.6945458402)`

:math:`bias = -0.3679223992 = -0.4259195021 + (1.0000000000)(0.0579971029)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0091119640 = (0.6945458402)(0.3054541598)((0.057997)(0.740557))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4568089487 = 0.4476969848 + 1.000000(0.0091119640)(1.0000000000)`

:math:`w_2 = 0.4568089487 = 0.4476969848 + 1.000000(0.0091119640)(1.0000000000)`

:math:`bias = -0.0648235587 = -0.0739355226 + (1.0000000000)(0.0091119640)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0091119640 = (0.6945458402)(0.3054541598)((0.057997)(0.740557))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4568089487 = 0.4476969848 + 1.000000(0.0091119640)(1.0000000000)`

:math:`w_2 = 0.4568089487 = 0.4476969848 + 1.000000(0.0091119640)(1.0000000000)`

:math:`bias = -0.0648235587 = -0.0739355226 + (1.0000000000)(0.0091119640)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.27320114226968206, 0.27320114226968206] -> [0.5146782430435879]

Error:  :math:`e = 0.050000 - 0.514678 = -0.4646782430`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1079629348`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.5146782430)(1 - 0.4853217570)(-0.4646782430) = -0.1160694455`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7491287553 = 0.7808390604 + 1.000000(-0.1160694455)(0.2732011423)`

:math:`w_2 = 0.7491287553 = 0.7808390604 + 1.000000(-0.1160694455)(0.2732011423)`

:math:`bias = -0.4839918446 = -0.3679223992 + (1.0000000000)(-0.1160694455)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0179960084 = (0.2732011423)(0.7267988577)((-0.116069)(0.780839))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4748049571 = 0.4568089487 + 1.000000(-0.0179960084)(-1.0000000000)`

:math:`w_2 = 0.4748049571 = 0.4568089487 + 1.000000(-0.0179960084)(-1.0000000000)`

:math:`bias = -0.0828195671 = -0.0648235587 + (1.0000000000)(-0.0179960084)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0179960084 = (0.2732011423)(0.7267988577)((-0.116069)(0.780839))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4748049571 = 0.4568089487 + 1.000000(-0.0179960084)(-1.0000000000)`

:math:`w_2 = 0.4748049571 = 0.4568089487 + 1.000000(-0.0179960084)(-1.0000000000)`

:math:`bias = -0.0828195671 = -0.0648235587 + (1.0000000000)(-0.0179960084)`

Iteration 7
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.7040773973885975, 0.7040773973885975] -> [0.6389702206418986]

Error:  :math:`e = 0.900000 - 0.638970 = 0.2610297794`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0340682729`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6389702206)(1 - 0.3610297794)(0.2610297794) = 0.0602162492`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7915256553 = 0.7491287553 + 1.000000(0.0602162492)(0.7040773974)`

:math:`w_2 = 0.7915256553 = 0.7491287553 + 1.000000(0.0602162492)(0.7040773974)`

:math:`bias = -0.4237755954 = -0.4839918446 + (1.0000000000)(0.0602162492)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0093987199 = (0.7040773974)(0.2959226026)((0.060216)(0.749129))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4842036770 = 0.4748049571 + 1.000000(0.0093987199)(1.0000000000)`

:math:`w_2 = 0.4842036770 = 0.4748049571 + 1.000000(0.0093987199)(1.0000000000)`

:math:`bias = -0.0734208471 = -0.0828195671 + (1.0000000000)(0.0093987199)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0093987199 = (0.7040773974)(0.2959226026)((0.060216)(0.749129))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4842036770 = 0.4748049571 + 1.000000(0.0093987199)(1.0000000000)`

:math:`w_2 = 0.4842036770 = 0.4748049571 + 1.000000(0.0093987199)(1.0000000000)`

:math:`bias = -0.0734208471 = -0.0828195671 + (1.0000000000)(0.0093987199)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.26079739533179674, 0.26079739533179674] -> [0.4972700428923381]

Error:  :math:`e = 0.050000 - 0.497270 = -0.4472700429`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1000252456`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4972700429)(1 - 0.5027299571)(-0.4472700429) = -0.1118141774`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7623648091 = 0.7915256553 + 1.000000(-0.1118141774)(0.2607973953)`

:math:`w_2 = 0.7623648091 = 0.7915256553 + 1.000000(-0.1118141774)(0.2607973953)`

:math:`bias = -0.5355897728 = -0.4237755954 + (1.0000000000)(-0.1118141774)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0170619477 = (0.2607973953)(0.7392026047)((-0.111814)(0.791526))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5012656248 = 0.4842036770 + 1.000000(-0.0170619477)(-1.0000000000)`

:math:`w_2 = 0.5012656248 = 0.4842036770 + 1.000000(-0.0170619477)(-1.0000000000)`

:math:`bias = -0.0904827948 = -0.0734208471 + (1.0000000000)(-0.0170619477)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0170619477 = (0.2607973953)(0.7392026047)((-0.111814)(0.791526))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5012656248 = 0.4842036770 + 1.000000(-0.0170619477)(-1.0000000000)`

:math:`w_2 = 0.5012656248 = 0.4842036770 + 1.000000(-0.0170619477)(-1.0000000000)`

:math:`bias = -0.0904827948 = -0.0734208471 + (1.0000000000)(-0.0170619477)`

Iteration 8
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.7134191569737081, 0.7134191569737081] -> [0.6346415780956359]

Error:  :math:`e = 0.900000 - 0.634642 = 0.2653584219`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0352075460`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6346415781)(1 - 0.3653584219)(0.2653584219) = 0.0615290939`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8062608434 = 0.7623648091 + 1.000000(0.0615290939)(0.7134191570)`

:math:`w_2 = 0.8062608434 = 0.7623648091 + 1.000000(0.0615290939)(0.7134191570)`

:math:`bias = -0.4740606789 = -0.5355897728 + (1.0000000000)(0.0615290939)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0095903683 = (0.7134191570)(0.2865808430)((0.061529)(0.762365))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5108559930 = 0.5012656248 + 1.000000(0.0095903683)(1.0000000000)`

:math:`w_2 = 0.5108559930 = 0.5012656248 + 1.000000(0.0095903683)(1.0000000000)`

:math:`bias = -0.0808924266 = -0.0904827948 + (1.0000000000)(0.0095903683)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0095903683 = (0.7134191570)(0.2865808430)((0.061529)(0.762365))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5108559930 = 0.5012656248 + 1.000000(0.0095903683)(1.0000000000)`

:math:`w_2 = 0.5108559930 = 0.5012656248 + 1.000000(0.0095903683)(1.0000000000)`

:math:`bias = -0.0808924266 = -0.0904827948 + (1.0000000000)(0.0095903683)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.24925222405712452, 0.24925222405712452] -> [0.48197380057567896]

Error:  :math:`e = 0.050000 - 0.481974 = -0.4319738006`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0933006822`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4819738006)(1 - 0.5180261994)(-0.4319738006) = -0.1078530829`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7793782226 = 0.8062608434 + 1.000000(-0.1078530829)(0.2492522241)`

:math:`w_2 = 0.7793782226 = 0.8062608434 + 1.000000(-0.1078530829)(0.2492522241)`

:math:`bias = -0.5819137618 = -0.4740606789 + (1.0000000000)(-0.1078530829)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0162720110 = (0.2492522241)(0.7507477759)((-0.107853)(0.806261))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5271280040 = 0.5108559930 + 1.000000(-0.0162720110)(-1.0000000000)`

:math:`w_2 = 0.5271280040 = 0.5108559930 + 1.000000(-0.0162720110)(-1.0000000000)`

:math:`bias = -0.0971644376 = -0.0808924266 + (1.0000000000)(-0.0162720110)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0162720110 = (0.2492522241)(0.7507477759)((-0.107853)(0.806261))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5271280040 = 0.5108559930 + 1.000000(-0.0162720110)(-1.0000000000)`

:math:`w_2 = 0.5271280040 = 0.5108559930 + 1.000000(-0.0162720110)(-1.0000000000)`

:math:`bias = -0.0971644376 = -0.0808924266 + (1.0000000000)(-0.0162720110)`

Iteration 9
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.7225391113437702, 0.7225391113437702] -> [0.6328234631782461]

Error:  :math:`e = 0.900000 - 0.632823 = 0.2671765368`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0356916509`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6328234632)(1 - 0.3671765368)(0.2671765368) = 0.0620805864`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8242338743 = 0.7793782226 + 1.000000(0.0620805864)(0.7225391113)`

:math:`w_2 = 0.8242338743 = 0.7793782226 + 1.000000(0.0620805864)(0.7225391113)`

:math:`bias = -0.5198331754 = -0.5819137618 + (1.0000000000)(0.0620805864)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0096998990 = (0.7225391113)(0.2774608887)((0.062081)(0.779378))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5368279030 = 0.5271280040 + 1.000000(0.0096998990)(1.0000000000)`

:math:`w_2 = 0.5368279030 = 0.5271280040 + 1.000000(0.0096998990)(1.0000000000)`

:math:`bias = -0.0874645386 = -0.0971644376 + (1.0000000000)(0.0096998990)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0096998990 = (0.7225391113)(0.2774608887)((0.062081)(0.779378))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5368279030 = 0.5271280040 + 1.000000(0.0096998990)(1.0000000000)`

:math:`w_2 = 0.5368279030 = 0.5271280040 + 1.000000(0.0096998990)(1.0000000000)`

:math:`bias = -0.0874645386 = -0.0971644376 + (1.0000000000)(0.0096998990)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.23846377232565563, 0.23846377232565563] -> [0.46835900403187974]

Error:  :math:`e = 0.050000 - 0.468359 = -0.4183590040`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0875121281`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4683590040)(1 - 0.5316409960)(-0.4183590040) = -0.1041709098`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7993928862 = 0.8242338743 + 1.000000(-0.1041709098)(0.2384637723)`

:math:`w_2 = 0.7993928862 = 0.8242338743 + 1.000000(-0.1041709098)(0.2384637723)`

:math:`bias = -0.6240040852 = -0.5198331754 + (1.0000000000)(-0.1041709098)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0155922897 = (0.2384637723)(0.7615362277)((-0.104171)(0.824234))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5524201926 = 0.5368279030 + 1.000000(-0.0155922897)(-1.0000000000)`

:math:`w_2 = 0.5524201926 = 0.5368279030 + 1.000000(-0.0155922897)(-1.0000000000)`

:math:`bias = -0.1030568283 = -0.0874645386 + (1.0000000000)(-0.0155922897)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0155922897 = (0.2384637723)(0.7615362277)((-0.104171)(0.824234))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5524201926 = 0.5368279030 + 1.000000(-0.0155922897)(-1.0000000000)`

:math:`w_2 = 0.5524201926 = 0.5368279030 + 1.000000(-0.0155922897)(-1.0000000000)`

:math:`bias = -0.1030568283 = -0.0874645386 + (1.0000000000)(-0.0155922897)`

Iteration 10
~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.7314091026674183, 0.7314091026674183] -> [0.6330589603851691]

Error:  :math:`e = 0.900000 - 0.633059 = 0.2669410396`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0356287593`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6330589604)(1 - 0.3669410396)(0.2669410396) = 0.0620091524`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8447469447 = 0.7993928862 + 1.000000(0.0620091524)(0.7314091027)`

:math:`w_2 = 0.8447469447 = 0.7993928862 + 1.000000(0.0620091524)(0.7314091027)`

:math:`bias = -0.5619949328 = -0.6240040852 + (1.0000000000)(0.0620091524)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0097379541 = (0.7314091027)(0.2685908973)((0.062009)(0.799393))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5621581468 = 0.5524201926 + 1.000000(0.0097379541)(1.0000000000)`

:math:`w_2 = 0.5621581468 = 0.5524201926 + 1.000000(0.0097379541)(1.0000000000)`

:math:`bias = -0.0933188741 = -0.1030568283 + (1.0000000000)(0.0097379541)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0097379541 = (0.7314091027)(0.2685908973)((0.062009)(0.799393))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5621581468 = 0.5524201926 + 1.000000(0.0097379541)(1.0000000000)`

:math:`w_2 = 0.5621581468 = 0.5524201926 + 1.000000(0.0097379541)(1.0000000000)`

:math:`bias = -0.0933188741 = -0.1030568283 + (1.0000000000)(0.0097379541)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.22835288486700145, 0.22835288486700145] -> [0.4560650699622863]

Error:  :math:`e = 0.050000 - 0.456065 = -0.4060650700`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0824444205`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4560650700)(1 - 0.5439349300)(-0.4060650700) = -0.1007324490`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8217443994 = 0.8447469447 + 1.000000(-0.1007324490)(0.2283528849)`

:math:`w_2 = 0.8217443994 = 0.8447469447 + 1.000000(-0.1007324490)(0.2283528849)`

:math:`bias = -0.6627273818 = -0.5619949328 + (1.0000000000)(-0.1007324490)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0149941296 = (0.2283528849)(0.7716471151)((-0.100732)(0.844747))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5771522764 = 0.5621581468 + 1.000000(-0.0149941296)(-1.0000000000)`

:math:`w_2 = 0.5771522764 = 0.5621581468 + 1.000000(-0.0149941296)(-1.0000000000)`

:math:`bias = -0.1083130038 = -0.0933188741 + (1.0000000000)(-0.0149941296)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0149941296 = (0.2283528849)(0.7716471151)((-0.100732)(0.844747))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5771522764 = 0.5621581468 + 1.000000(-0.0149941296)(-1.0000000000)`

:math:`w_2 = 0.5771522764 = 0.5621581468 + 1.000000(-0.0149941296)(-1.0000000000)`

:math:`bias = -0.1083130038 = -0.0933188741 + (1.0000000000)(-0.0149941296)`

Iteration 11
~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.7400044239952496, 0.7400044239952496] -> [0.6349383351573638]

Error:  :math:`e = 0.900000 - 0.634938 = 0.2650616648`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0351288431`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6349383352)(1 - 0.3650616648)(0.2650616648) = 0.0614390795`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8672095900 = 0.8217443994 + 1.000000(0.0614390795)(0.7400044240)`

:math:`w_2 = 0.8672095900 = 0.8217443994 + 1.000000(0.0614390795)(0.7400044240)`

:math:`bias = -0.6012883023 = -0.6627273818 + (1.0000000000)(0.0614390795)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0097136338 = (0.7400044240)(0.2599955760)((0.061439)(0.821744))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5868659102 = 0.5771522764 + 1.000000(0.0097136338)(1.0000000000)`

:math:`w_2 = 0.5868659102 = 0.5771522764 + 1.000000(0.0097136338)(1.0000000000)`

:math:`bias = -0.0985993700 = -0.1083130038 + (1.0000000000)(0.0097136338)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0097136338 = (0.7400044240)(0.2599955760)((0.061439)(0.821744))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5868659102 = 0.5771522764 + 1.000000(0.0097136338)(1.0000000000)`

:math:`w_2 = 0.5868659102 = 0.5771522764 + 1.000000(0.0097136338)(1.0000000000)`

:math:`bias = -0.0985993700 = -0.1083130038 + (1.0000000000)(0.0097136338)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.21885845190619235, 0.21885845190619235] -> [0.444801891829075]

Error:  :math:`e = 0.050000 - 0.444802 = -0.3948018918`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0779342669`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4448018918)(1 - 0.5551981082)(-0.3948018918) = -0.0974975783`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8458714210 = 0.8672095900 + 1.000000(-0.0974975783)(0.2188584519)`

:math:`w_2 = 0.8458714210 = 0.8672095900 + 1.000000(-0.0974975783)(0.2188584519)`

:math:`bias = -0.6987858805 = -0.6012883023 + (1.0000000000)(-0.0974975783)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0144547625 = (0.2188584519)(0.7811415481)((-0.097498)(0.867210))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6013206728 = 0.5868659102 + 1.000000(-0.0144547625)(-1.0000000000)`

:math:`w_2 = 0.6013206728 = 0.5868659102 + 1.000000(-0.0144547625)(-1.0000000000)`

:math:`bias = -0.1130541325 = -0.0985993700 + (1.0000000000)(-0.0144547625)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0144547625 = (0.2188584519)(0.7811415481)((-0.097498)(0.867210))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6013206728 = 0.5868659102 + 1.000000(-0.0144547625)(-1.0000000000)`

:math:`w_2 = 0.6013206728 = 0.5868659102 + 1.000000(-0.0144547625)(-1.0000000000)`

:math:`bias = -0.1130541325 = -0.0985993700 + (1.0000000000)(-0.0144547625)`

Iteration 12
~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.7483039831601918, 0.7483039831601918] -> [0.6381057607888653]

Error:  :math:`e = 0.900000 - 0.638106 = 0.2618942392`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0342942963`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6381057608)(1 - 0.3618942392)(0.2618942392) = 0.0604783983`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8911276473 = 0.8458714210 + 1.000000(0.0604783983)(0.7483039832)`

:math:`w_2 = 0.8911276473 = 0.8458714210 + 1.000000(0.0604783983)(0.7483039832)`

:math:`bias = -0.6383074822 = -0.6987858805 + (1.0000000000)(0.0604783983)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0096351623 = (0.7483039832)(0.2516960168)((0.060478)(0.845871))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6109558350 = 0.6013206728 + 1.000000(0.0096351623)(1.0000000000)`

:math:`w_2 = 0.6109558350 = 0.6013206728 + 1.000000(0.0096351623)(1.0000000000)`

:math:`bias = -0.1034189703 = -0.1130541325 + (1.0000000000)(0.0096351623)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0096351623 = (0.7483039832)(0.2516960168)((0.060478)(0.845871))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6109558350 = 0.6013206728 + 1.000000(0.0096351623)(1.0000000000)`

:math:`w_2 = 0.6109558350 = 0.6013206728 + 1.000000(0.0096351623)(1.0000000000)`

:math:`bias = -0.1034189703 = -0.1130541325 + (1.0000000000)(0.0096351623)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.20993278098104917, 0.20993278098104917] -> [0.4343429191889135]

Error:  :math:`e = 0.050000 - 0.434343 = -0.3843429192`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0738597398`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4343429192)(1 - 0.5656570808)(-0.3843429192) = -0.0944288843`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8713039291 = 0.8911276473 + 1.000000(-0.0944288843)(0.2099327810)`

:math:`w_2 = 0.8713039291 = 0.8911276473 + 1.000000(-0.0944288843)(0.2099327810)`

:math:`bias = -0.7327363665 = -0.6383074822 + (1.0000000000)(-0.0944288843)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0139569036 = (0.2099327810)(0.7900672190)((-0.094429)(0.891128))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6249127386 = 0.6109558350 + 1.000000(-0.0139569036)(-1.0000000000)`

:math:`w_2 = 0.6249127386 = 0.6109558350 + 1.000000(-0.0139569036)(-1.0000000000)`

:math:`bias = -0.1173758738 = -0.1034189703 + (1.0000000000)(-0.0139569036)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0139569036 = (0.2099327810)(0.7900672190)((-0.094429)(0.891128))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6249127386 = 0.6109558350 + 1.000000(-0.0139569036)(-1.0000000000)`

:math:`w_2 = 0.6249127386 = 0.6109558350 + 1.000000(-0.0139569036)(-1.0000000000)`

:math:`bias = -0.1173758738 = -0.1034189703 + (1.0000000000)(-0.0139569036)`

Iteration 13
~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.756290681357725, 0.756290681357725] -> [0.6422588459704319]

Error:  :math:`e = 0.900000 - 0.642259 = 0.2577411540`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0332152512`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6422588460)(1 - 0.3577411540)(0.2577411540) = 0.0592192315`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9160908820 = 0.8713039291 + 1.000000(0.0592192315)(0.7562906814)`

:math:`w_2 = 0.9160908820 = 0.8713039291 + 1.000000(0.0592192315)(0.7562906814)`

:math:`bias = -0.6735171350 = -0.7327363665 + (1.0000000000)(0.0592192315)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0095102805 = (0.7562906814)(0.2437093186)((0.059219)(0.871304))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6344230190 = 0.6249127386 + 1.000000(0.0095102805)(1.0000000000)`

:math:`w_2 = 0.6344230190 = 0.6249127386 + 1.000000(0.0095102805)(1.0000000000)`

:math:`bias = -0.1078655934 = -0.1173758738 + (1.0000000000)(0.0095102805)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0095102805 = (0.7562906814)(0.2437093186)((0.059219)(0.871304))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6344230190 = 0.6249127386 + 1.000000(0.0095102805)(1.0000000000)`

:math:`w_2 = 0.6344230190 = 0.6249127386 + 1.000000(0.0095102805)(1.0000000000)`

:math:`bias = -0.1078655934 = -0.1173758738 + (1.0000000000)(0.0095102805)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.20153764543421823, 0.20153764543421823] -> [0.42451555857390155]

Error:  :math:`e = 0.050000 - 0.424516 = -0.3745155586`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0701309518`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4245155586)(1 - 0.5754844414)(-0.3745155586) = -0.0914949371`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8976512078 = 0.9160908820 + 1.000000(-0.0914949371)(0.2015376454)`

:math:`w_2 = 0.8976512078 = 0.9160908820 + 1.000000(-0.0914949371)(0.2015376454)`

:math:`bias = -0.7650120721 = -0.6735171350 + (1.0000000000)(-0.0914949371)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0134879594 = (0.2015376454)(0.7984623546)((-0.091495)(0.916091))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6479109784 = 0.6344230190 + 1.000000(-0.0134879594)(-1.0000000000)`

:math:`w_2 = 0.6479109784 = 0.6344230190 + 1.000000(-0.0134879594)(-1.0000000000)`

:math:`bias = -0.1213535527 = -0.1078655934 + (1.0000000000)(-0.0134879594)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0134879594 = (0.2015376454)(0.7984623546)((-0.091495)(0.916091))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6479109784 = 0.6344230190 + 1.000000(-0.0134879594)(-1.0000000000)`

:math:`w_2 = 0.6479109784 = 0.6344230190 + 1.000000(-0.0134879594)(-1.0000000000)`

:math:`bias = -0.1213535527 = -0.1078655934 + (1.0000000000)(-0.0134879594)`

Iteration 14
~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.7639517508368617, 0.7639517508368617] -> [0.6471448124175178]

Error:  :math:`e = 0.900000 - 0.647145 = 0.2528551876`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0319678729`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6471448124)(1 - 0.3528551876)(0.2528551876) = 0.0577390786`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9417610780 = 0.8976512078 + 1.000000(0.0577390786)(0.7639517508)`

:math:`w_2 = 0.9417610780 = 0.8976512078 + 1.000000(0.0577390786)(0.7639517508)`

:math:`bias = -0.7072729936 = -0.7650120721 + (1.0000000000)(0.0577390786)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0093463961 = (0.7639517508)(0.2360482492)((0.057739)(0.897651))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6572573745 = 0.6479109784 + 1.000000(0.0093463961)(1.0000000000)`

:math:`w_2 = 0.6572573745 = 0.6479109784 + 1.000000(0.0093463961)(1.0000000000)`

:math:`bias = -0.1120071566 = -0.1213535527 + (1.0000000000)(0.0093463961)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0093463961 = (0.7639517508)(0.2360482492)((0.057739)(0.897651))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6572573745 = 0.6479109784 + 1.000000(0.0093463961)(1.0000000000)`

:math:`w_2 = 0.6572573745 = 0.6479109784 + 1.000000(0.0093463961)(1.0000000000)`

:math:`bias = -0.1120071566 = -0.1213535527 + (1.0000000000)(0.0093463961)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.19364119015942768, 0.19364119015942768] -> [0.41519127225581376]

Error:  :math:`e = 0.050000 - 0.415191 = -0.3651912723`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0666823327`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4151912723)(1 - 0.5848087277)(-0.3651912723) = -0.0886711724`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9245906866 = 0.9417610780 + 1.000000(-0.0886711724)(0.1936411902)`

:math:`w_2 = 0.9245906866 = 0.9417610780 + 1.000000(-0.0886711724)(0.1936411902)`

:math:`bias = -0.7959441660 = -0.7072729936 + (1.0000000000)(-0.0886711724)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0130391496 = (0.1936411902)(0.8063588098)((-0.088671)(0.941761))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6702965241 = 0.6572573745 + 1.000000(-0.0130391496)(-1.0000000000)`

:math:`w_2 = 0.6702965241 = 0.6572573745 + 1.000000(-0.0130391496)(-1.0000000000)`

:math:`bias = -0.1250463062 = -0.1120071566 + (1.0000000000)(-0.0130391496)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0130391496 = (0.1936411902)(0.8063588098)((-0.088671)(0.941761))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6702965241 = 0.6572573745 + 1.000000(-0.0130391496)(-1.0000000000)`

:math:`w_2 = 0.6702965241 = 0.6572573745 + 1.000000(-0.0130391496)(-1.0000000000)`

:math:`bias = -0.1250463062 = -0.1120071566 + (1.0000000000)(-0.0130391496)`

Iteration 15
~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.7712789093792612, 0.7712789093792612] -> [0.6525553126644273]

Error:  :math:`e = 0.900000 - 0.652555 = 0.2474446873`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0306144366`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6525553127)(1 - 0.3474446873)(0.2474446873) = 0.0561023611`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9678612545 = 0.9245906866 + 1.000000(0.0561023611)(0.7712789094)`

:math:`w_2 = 0.9678612545 = 0.9245906866 + 1.000000(0.0561023611)(0.7712789094)`

:math:`bias = -0.7398418049 = -0.7959441660 + (1.0000000000)(0.0561023611)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0091505737 = (0.7712789094)(0.2287210906)((0.056102)(0.924591))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6794470978 = 0.6702965241 + 1.000000(0.0091505737)(1.0000000000)`

:math:`w_2 = 0.6794470978 = 0.6702965241 + 1.000000(0.0091505737)(1.0000000000)`

:math:`bias = -0.1158957325 = -0.1250463062 + (1.0000000000)(0.0091505737)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0091505737 = (0.7712789094)(0.2287210906)((0.056102)(0.924591))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6794470978 = 0.6702965241 + 1.000000(0.0091505737)(1.0000000000)`

:math:`w_2 = 0.6794470978 = 0.6702965241 + 1.000000(0.0091505737)(1.0000000000)`

:math:`bias = -0.1158957325 = -0.1250463062 + (1.0000000000)(0.0091505737)`

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.18621565998386339, 0.18621565998386339] -> [0.406276451790183]

Error:  :math:`e = 0.050000 - 0.406276 = -0.3562764518`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0634664551`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4062764518)(1 - 0.5937235482)(-0.3562764518) = -0.0859395437`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9518579656 = 0.9678612545 + 1.000000(-0.0859395437)(0.1862156600)`

:math:`w_2 = 0.9518579656 = 0.9678612545 + 1.000000(-0.0859395437)(0.1862156600)`

:math:`bias = -0.8257813486 = -0.7398418049 + (1.0000000000)(-0.0859395437)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0126046757 = (0.1862156600)(0.8137843400)((-0.085940)(0.967861))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6920517735 = 0.6794470978 + 1.000000(-0.0126046757)(-1.0000000000)`

:math:`w_2 = 0.6920517735 = 0.6794470978 + 1.000000(-0.0126046757)(-1.0000000000)`

:math:`bias = -0.1285004082 = -0.1158957325 + (1.0000000000)(-0.0126046757)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0126046757 = (0.1862156600)(0.8137843400)((-0.085940)(0.967861))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.6920517735 = 0.6794470978 + 1.000000(-0.0126046757)(-1.0000000000)`

:math:`w_2 = 0.6920517735 = 0.6794470978 + 1.000000(-0.0126046757)(-1.0000000000)`

:math:`bias = -0.1285004082 = -0.1158957325 + (1.0000000000)(-0.0126046757)`


.. _alternating-training-final:

Final Network
~~~~~~~~~~~~~~~

.. graphviz::

    digraph network{
    rankdir=LR;
    node [shape=plaintext,width=0.25,label=<x<SUB>1</SUB>>]; x_1;
    node [shape=plaintext,width=0.25,label=<x<SUB>2</SUB>>]; x_2;
    node [shape=square,width=.25,fillcolor=black,style=filled,label=""]; input_0, input_1
    node [shape=circle,width=.5,fillcolor=white,label="-0.128500"]; h_0_0;
    node [shape=circle,width=.5,fillcolor=white,label="-0.128500"]; h_0_1;
    node [shape=circle,width=.5,fillcolor=white,label="-0.825781"]; o_0;
    node [shape=circle,width=.5,style=invisible,label=""]; output;
    
    
    x_1 -> input_0;
    x_2 -> input_1;
    input_0 -> h_0_0 [label="0.692052"];
    input_1 -> h_0_0 [label="0.692052"];
    input_0 -> h_0_1 [label="0.692052"];
    input_1 -> h_0_1 [label="0.692052"];
    h_0_0 -> o_0 [label="0.951858"];
    h_0_1 -> o_0 [label="0.951858"];
    o_0 -> output [label="Output"];
    }
    


.. _alternating-validation-test:

Alternating Training Validation Test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

[1, 1] -> [0.7782682840880495, 0.7782682840880495] -> [0.6583208713508027]

Error:  :math:`e = 0.900000 - 0.658321 = 0.2416791286`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0292044006`

0.6583208713508027

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

[-1, -1] -> [0.18055320770809843, 0.18055320770809843] -> [0.38176596233785315]

Error:  :math:`e = 0.050000 - 0.381766 = -0.3317659623`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0550343269`

0.38176596233785315

Successive Training Method
--------------------------


For each input/output pair in the training set, present the pair and perform the back propagation
technique to update the weights for the specified number of iterations. Then move onto the next
input/output pair and repeat until no more training data is available.

These are the results of executing 15 iterations of training with this method with the following
inputs:

.. math:: ( 1, 1) \rightarrow 0.9

.. math:: (-1, -1) \rightarrow 0.05


.. contents:: Iterations
    :depth: 2
    :local:


Input 1 - Iteration 1
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6456563062257954, 0.6456563062257954] -> [0.7375067919437424]

Error:  :math:`e = 0.900000 - 0.737507 = 0.1624932081`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0132020213`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7375067919)(1 - 0.2624932081)(0.1624932081) = 0.0314571453`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8203105042 = 0.8000000000 + 1.000000(0.0314571453)(0.6456563062)`

:math:`w_2 = 0.8203105042 = 0.8000000000 + 1.000000(0.0314571453)(0.6456563062)`

:math:`bias = 0.0314571453 = 0.0000000000 + (1.0000000000)(0.0314571453)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0057575193 = (0.6456563062)(0.3543436938)((0.031457)(0.800000))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3057575193 = 0.3000000000 + 1.000000(0.0057575193)(1.0000000000)`

:math:`w_2 = 0.3057575193 = 0.3000000000 + 1.000000(0.0057575193)(1.0000000000)`

:math:`bias = 0.0057575193 = 0.0000000000 + (1.0000000000)(0.0057575193)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0057575193 = (0.6456563062)(0.3543436938)((0.031457)(0.800000))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3057575193 = 0.3000000000 + 1.000000(0.0057575193)(1.0000000000)`

:math:`w_2 = 0.3057575193 = 0.3000000000 + 1.000000(0.0057575193)(1.0000000000)`

:math:`bias = 0.0057575193 = 0.0000000000 + (1.0000000000)(0.0057575193)`

Input 1 - Iteration 2
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6495979805583044, 0.6495979805583044] -> [0.7497353349032734]

Error:  :math:`e = 0.900000 - 0.749735 = 0.1502646651`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0112897348`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7497353349)(1 - 0.2502646651)(0.1502646651) = 0.0281944991`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8386255939 = 0.8203105042 + 1.000000(0.0281944991)(0.6495979806)`

:math:`w_2 = 0.8386255939 = 0.8203105042 + 1.000000(0.0281944991)(0.6495979806)`

:math:`bias = 0.0596516443 = 0.0314571453 + (1.0000000000)(0.0281944991)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0052644611 = (0.6495979806)(0.3504020194)((0.028194)(0.820311))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3110219804 = 0.3057575193 + 1.000000(0.0052644611)(1.0000000000)`

:math:`w_2 = 0.3110219804 = 0.3057575193 + 1.000000(0.0052644611)(1.0000000000)`

:math:`bias = 0.0110219804 = 0.0057575193 + (1.0000000000)(0.0052644611)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0052644611 = (0.6495979806)(0.3504020194)((0.028194)(0.820311))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3110219804 = 0.3057575193 + 1.000000(0.0052644611)(1.0000000000)`

:math:`w_2 = 0.3110219804 = 0.3057575193 + 1.000000(0.0052644611)(1.0000000000)`

:math:`bias = 0.0110219804 = 0.0057575193 + (1.0000000000)(0.0052644611)`

Input 1 - Iteration 3
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6531843296310355, 0.6531843296310355] -> [0.760460499342739]

Error:  :math:`e = 0.900000 - 0.760460 = 0.1395395007`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0097356361`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7604604993)(1 - 0.2395395007)(0.1395395007) = 0.0254185612`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8552285998 = 0.8386255939 + 1.000000(0.0254185612)(0.6531843296)`

:math:`w_2 = 0.8552285998 = 0.8386255939 + 1.000000(0.0254185612)(0.6531843296)`

:math:`bias = 0.0850702056 = 0.0596516443 + (1.0000000000)(0.0254185612)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0048289593 = (0.6531843296)(0.3468156704)((0.025419)(0.838626))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3158509397 = 0.3110219804 + 1.000000(0.0048289593)(1.0000000000)`

:math:`w_2 = 0.3158509397 = 0.3110219804 + 1.000000(0.0048289593)(1.0000000000)`

:math:`bias = 0.0158509397 = 0.0110219804 + (1.0000000000)(0.0048289593)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0048289593 = (0.6531843296)(0.3468156704)((0.025419)(0.838626))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3158509397 = 0.3110219804 + 1.000000(0.0048289593)(1.0000000000)`

:math:`w_2 = 0.3158509397 = 0.3110219804 + 1.000000(0.0048289593)(1.0000000000)`

:math:`bias = 0.0158509397 = 0.0110219804 + (1.0000000000)(0.0048289593)`

Input 1 - Iteration 4
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.656458784355454, 0.656458784355454] -> [0.7699297995357816]

Error:  :math:`e = 0.900000 - 0.769930 = 0.1300702005`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0084591285`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7699297995)(1 - 0.2300702005)(0.1300702005) = 0.0230403626`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8703536482 = 0.8552285998 + 1.000000(0.0230403626)(0.6564587844)`

:math:`w_2 = 0.8703536482 = 0.8552285998 + 1.000000(0.0230403626)(0.6564587844)`

:math:`bias = 0.1081105682 = 0.0850702056 + (1.0000000000)(0.0230403626)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0044438341 = (0.6564587844)(0.3435412156)((0.023040)(0.855229))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3202947738 = 0.3158509397 + 1.000000(0.0044438341)(1.0000000000)`

:math:`w_2 = 0.3202947738 = 0.3158509397 + 1.000000(0.0044438341)(1.0000000000)`

:math:`bias = 0.0202947738 = 0.0158509397 + (1.0000000000)(0.0044438341)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0044438341 = (0.6564587844)(0.3435412156)((0.023040)(0.855229))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3202947738 = 0.3158509397 + 1.000000(0.0044438341)(1.0000000000)`

:math:`w_2 = 0.3202947738 = 0.3158509397 + 1.000000(0.0044438341)(1.0000000000)`

:math:`bias = 0.0202947738 = 0.0158509397 + (1.0000000000)(0.0044438341)`

Input 1 - Iteration 5
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6594590110051756, 0.6594590110051756] -> [0.7783429174060408]

Error:  :math:`e = 0.900000 - 0.778343 = 0.1216570826`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0074002229`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7783429174)(1 - 0.2216570826)(0.1216570826) = 0.0209889150`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8841949773 = 0.8703536482 + 1.000000(0.0209889150)(0.6594590110)`

:math:`w_2 = 0.8841949773 = 0.8703536482 + 1.000000(0.0209889150)(0.6594590110)`

:math:`bias = 0.1290994832 = 0.1081105682 + (1.0000000000)(0.0209889150)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0041024467 = (0.6594590110)(0.3405409890)((0.020989)(0.870354))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3243972205 = 0.3202947738 + 1.000000(0.0041024467)(1.0000000000)`

:math:`w_2 = 0.3243972205 = 0.3202947738 + 1.000000(0.0041024467)(1.0000000000)`

:math:`bias = 0.0243972205 = 0.0202947738 + (1.0000000000)(0.0041024467)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0041024467 = (0.6594590110)(0.3405409890)((0.020989)(0.870354))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3243972205 = 0.3202947738 + 1.000000(0.0041024467)(1.0000000000)`

:math:`w_2 = 0.3243972205 = 0.3202947738 + 1.000000(0.0041024467)(1.0000000000)`

:math:`bias = 0.0243972205 = 0.0202947738 + (1.0000000000)(0.0041024467)`

Input 1 - Iteration 6
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6622174567799363, 0.6622174567799363] -> [0.7858616035268302]

Error:  :math:`e = 0.900000 - 0.785862 = 0.1141383965`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0065137868`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7858616035)(1 - 0.2141383965)(0.1141383965) = 0.0192075682`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8969145642 = 0.8841949773 + 1.000000(0.0192075682)(0.6622174568)`

:math:`w_2 = 0.8969145642 = 0.8841949773 + 1.000000(0.0192075682)(0.6622174568)`

:math:`bias = 0.1483070513 = 0.1290994832 + (1.0000000000)(0.0192075682)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0037989034 = (0.6622174568)(0.3377825432)((0.019208)(0.884195))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3281961239 = 0.3243972205 + 1.000000(0.0037989034)(1.0000000000)`

:math:`w_2 = 0.3281961239 = 0.3243972205 + 1.000000(0.0037989034)(1.0000000000)`

:math:`bias = 0.0281961239 = 0.0243972205 + (1.0000000000)(0.0037989034)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0037989034 = (0.6622174568)(0.3377825432)((0.019208)(0.884195))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3281961239 = 0.3243972205 + 1.000000(0.0037989034)(1.0000000000)`

:math:`w_2 = 0.3281961239 = 0.3243972205 + 1.000000(0.0037989034)(1.0000000000)`

:math:`bias = 0.0281961239 = 0.0243972205 + (1.0000000000)(0.0037989034)`

Input 1 - Iteration 7
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6647620038169657, 0.6647620038169657] -> [0.792617607135255]

Error:  :math:`e = 0.900000 - 0.792618 = 0.1073823929`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0057654891`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7926176071)(1 - 0.2073823929)(0.1073823929) = 0.0176509740`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9086482611 = 0.8969145642 + 1.000000(0.0176509740)(0.6647620038)`

:math:`w_2 = 0.9086482611 = 0.8969145642 + 1.000000(0.0176509740)(0.6647620038)`

:math:`bias = 0.1659580253 = 0.1483070513 + (1.0000000000)(0.0176509740)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0035280861 = (0.6647620038)(0.3352379962)((0.017651)(0.896915))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3317242100 = 0.3281961239 + 1.000000(0.0035280861)(1.0000000000)`

:math:`w_2 = 0.3317242100 = 0.3281961239 + 1.000000(0.0035280861)(1.0000000000)`

:math:`bias = 0.0317242100 = 0.0281961239 + (1.0000000000)(0.0035280861)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0035280861 = (0.6647620038)(0.3352379962)((0.017651)(0.896915))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3317242100 = 0.3281961239 + 1.000000(0.0035280861)(1.0000000000)`

:math:`w_2 = 0.3317242100 = 0.3281961239 + 1.000000(0.0035280861)(1.0000000000)`

:math:`bias = 0.0317242100 = 0.0281961239 + (1.0000000000)(0.0035280861)`

Input 1 - Iteration 8
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6671166144832759, 0.6671166144832759] -> [0.7987189157655279]

Error:  :math:`e = 0.900000 - 0.798719 = 0.1012810842`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0051289290`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7987189158)(1 - 0.2012810842)(0.1012810842) = 0.0162826570`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9195106921 = 0.9086482611 + 1.000000(0.0162826570)(0.6671166145)`

:math:`w_2 = 0.9195106921 = 0.9086482611 + 1.000000(0.0162826570)(0.6671166145)`

:math:`bias = 0.1822406823 = 0.1659580253 + (1.0000000000)(0.0162826570)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0032856020 = (0.6671166145)(0.3328833855)((0.016283)(0.908648))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3350098120 = 0.3317242100 + 1.000000(0.0032856020)(1.0000000000)`

:math:`w_2 = 0.3350098120 = 0.3317242100 + 1.000000(0.0032856020)(1.0000000000)`

:math:`bias = 0.0350098120 = 0.0317242100 + (1.0000000000)(0.0032856020)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0032856020 = (0.6671166145)(0.3328833855)((0.016283)(0.908648))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3350098120 = 0.3317242100 + 1.000000(0.0032856020)(1.0000000000)`

:math:`w_2 = 0.3350098120 = 0.3317242100 + 1.000000(0.0032856020)(1.0000000000)`

:math:`bias = 0.0350098120 = 0.0317242100 + (1.0000000000)(0.0032856020)`

Input 1 - Iteration 9
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6693019180527188, 0.6693019180527188] -> [0.8042546270130423]

Error:  :math:`e = 0.900000 - 0.804255 = 0.0957453730`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0045835882`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.8042546270)(1 - 0.1957453730)(0.0957453730) = 0.0150731100`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9295991535 = 0.9195106921 + 1.000000(0.0150731100)(0.6693019181)`

:math:`w_2 = 0.9295991535 = 0.9195106921 + 1.000000(0.0150731100)(0.6693019181)`

:math:`bias = 0.1973137923 = 0.1822406823 + (1.0000000000)(0.0150731100)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0030677036 = (0.6693019181)(0.3306980819)((0.015073)(0.919511))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3380775156 = 0.3350098120 + 1.000000(0.0030677036)(1.0000000000)`

:math:`w_2 = 0.3380775156 = 0.3350098120 + 1.000000(0.0030677036)(1.0000000000)`

:math:`bias = 0.0380775156 = 0.0350098120 + (1.0000000000)(0.0030677036)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0030677036 = (0.6693019181)(0.3306980819)((0.015073)(0.919511))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3380775156 = 0.3350098120 + 1.000000(0.0030677036)(1.0000000000)`

:math:`w_2 = 0.3380775156 = 0.3350098120 + 1.000000(0.0030677036)(1.0000000000)`

:math:`bias = 0.0380775156 = 0.0350098120 + (1.0000000000)(0.0030677036)`

Input 1 - Iteration 10
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6713357224782142, 0.6713357224782142] -> [0.8092987448173119]

Error:  :math:`e = 0.900000 - 0.809299 = 0.0907012552`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0041133588`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.8092987448)(1 - 0.1907012552)(0.0907012552) = 0.0139983135`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9389967214 = 0.9295991535 + 1.000000(0.0139983135)(0.6713357225)`

:math:`w_2 = 0.9389967214 = 0.9295991535 + 1.000000(0.0139983135)(0.6713357225)`

:math:`bias = 0.2113121058 = 0.1973137923 + (1.0000000000)(0.0139983135)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0028712017 = (0.6713357225)(0.3286642775)((0.013998)(0.929599))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3409487172 = 0.3380775156 + 1.000000(0.0028712017)(1.0000000000)`

:math:`w_2 = 0.3409487172 = 0.3380775156 + 1.000000(0.0028712017)(1.0000000000)`

:math:`bias = 0.0409487172 = 0.0380775156 + (1.0000000000)(0.0028712017)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0028712017 = (0.6713357225)(0.3286642775)((0.013998)(0.929599))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3409487172 = 0.3380775156 + 1.000000(0.0028712017)(1.0000000000)`

:math:`w_2 = 0.3409487172 = 0.3380775156 + 1.000000(0.0028712017)(1.0000000000)`

:math:`bias = 0.0409487172 = 0.0380775156 + (1.0000000000)(0.0028712017)`

Input 1 - Iteration 11
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.673233450900169, 0.673233450900169] -> [0.8139131420244777]

Error:  :math:`e = 0.900000 - 0.813913 = 0.0860868580`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0037054736`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.8139131420)(1 - 0.1860868580)(0.0860868580) = 0.0130385898`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9477747362 = 0.9389967214 + 1.000000(0.0130385898)(0.6732334509)`

:math:`w_2 = 0.9477747362 = 0.9389967214 + 1.000000(0.0130385898)(0.6732334509)`

:math:`bias = 0.2243506956 = 0.2113121058 + (1.0000000000)(0.0130385898)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0026933821 = (0.6732334509)(0.3267665491)((0.013039)(0.938997))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3436420994 = 0.3409487172 + 1.000000(0.0026933821)(1.0000000000)`

:math:`w_2 = 0.3436420994 = 0.3409487172 + 1.000000(0.0026933821)(1.0000000000)`

:math:`bias = 0.0436420994 = 0.0409487172 + (1.0000000000)(0.0026933821)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0026933821 = (0.6732334509)(0.3267665491)((0.013039)(0.938997))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3436420994 = 0.3409487172 + 1.000000(0.0026933821)(1.0000000000)`

:math:`w_2 = 0.3436420994 = 0.3409487172 + 1.000000(0.0026933821)(1.0000000000)`

:math:`bias = 0.0436420994 = 0.0409487172 + (1.0000000000)(0.0026933821)`

Input 1 - Iteration 12
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6750085093951187, 0.6750085093951187] -> [0.8181498805256898]

Error:  :math:`e = 0.900000 - 0.818150 = 0.0818501195`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0033497210`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.8181498805)(1 - 0.1818501195)(0.0818501195) = 0.0121777143`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9559947970 = 0.9477747362 + 1.000000(0.0121777143)(0.6750085094)`

:math:`w_2 = 0.9559947970 = 0.9477747362 + 1.000000(0.0121777143)(0.6750085094)`

:math:`bias = 0.2365284098 = 0.2243506956 + (1.0000000000)(0.0121777143)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0025319326 = (0.6750085094)(0.3249914906)((0.012178)(0.947775))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3461740320 = 0.3436420994 + 1.000000(0.0025319326)(1.0000000000)`

:math:`w_2 = 0.3461740320 = 0.3436420994 + 1.000000(0.0025319326)(1.0000000000)`

:math:`bias = 0.0461740320 = 0.0436420994 + (1.0000000000)(0.0025319326)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0025319326 = (0.6750085094)(0.3249914906)((0.012178)(0.947775))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3461740320 = 0.3436420994 + 1.000000(0.0025319326)(1.0000000000)`

:math:`w_2 = 0.3461740320 = 0.3436420994 + 1.000000(0.0025319326)(1.0000000000)`

:math:`bias = 0.0461740320 = 0.0436420994 + (1.0000000000)(0.0025319326)`

Input 1 - Iteration 13
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6766725948139414, 0.6766725948139414] -> [0.8220530369926127]

Error:  :math:`e = 0.900000 - 0.822053 = 0.0779469630`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0030378645`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.8220530370)(1 - 0.1779469630)(0.0779469630) = 0.0114022253`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9637103703 = 0.9559947970 + 1.000000(0.0114022253)(0.6766725948)`

:math:`w_2 = 0.9637103703 = 0.9559947970 + 1.000000(0.0114022253)(0.6766725948)`

:math:`bias = 0.2479306351 = 0.2365284098 + (1.0000000000)(0.0114022253)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0023848785 = (0.6766725948)(0.3233274052)((0.011402)(0.955995))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3485589104 = 0.3461740320 + 1.000000(0.0023848785)(1.0000000000)`

:math:`w_2 = 0.3485589104 = 0.3461740320 + 1.000000(0.0023848785)(1.0000000000)`

:math:`bias = 0.0485589104 = 0.0461740320 + (1.0000000000)(0.0023848785)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0023848785 = (0.6766725948)(0.3233274052)((0.011402)(0.955995))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3485589104 = 0.3461740320 + 1.000000(0.0023848785)(1.0000000000)`

:math:`w_2 = 0.3485589104 = 0.3461740320 + 1.000000(0.0023848785)(1.0000000000)`

:math:`bias = 0.0485589104 = 0.0461740320 + (1.0000000000)(0.0023848785)`

Input 1 - Iteration 14
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.6782359517555755, 0.6782359517555755] -> [0.8256601474940775]

Error:  :math:`e = 0.900000 - 0.825660 = 0.0743398525`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0027632068`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.8256601475)(1 - 0.1743398525)(0.0743398525) = 0.0107008849`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9709680952 = 0.9637103703 + 1.000000(0.0107008849)(0.6782359518)`

:math:`w_2 = 0.9709680952 = 0.9637103703 + 1.000000(0.0107008849)(0.6782359518)`

:math:`bias = 0.2586315200 = 0.2479306351 + (1.0000000000)(0.0107008849)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0022505287 = (0.6782359518)(0.3217640482)((0.010701)(0.963710))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3508094391 = 0.3485589104 + 1.000000(0.0022505287)(1.0000000000)`

:math:`w_2 = 0.3508094391 = 0.3485589104 + 1.000000(0.0022505287)(1.0000000000)`

:math:`bias = 0.0508094391 = 0.0485589104 + (1.0000000000)(0.0022505287)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0022505287 = (0.6782359518)(0.3217640482)((0.010701)(0.963710))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3508094391 = 0.3485589104 + 1.000000(0.0022505287)(1.0000000000)`

:math:`w_2 = 0.3508094391 = 0.3485589104 + 1.000000(0.0022505287)(1.0000000000)`

:math:`bias = 0.0508094391 = 0.0485589104 + (1.0000000000)(0.0022505287)`

Input 1 - Iteration 15
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 1] -> [0.679707586982503, 0.679707586982503] -> [0.8290033573043031]

Error:  :math:`e = 0.900000 - 0.829003 = 0.0709966427`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0025202616`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.8290033573)(1 - 0.1709966427)(0.0709966427) = 0.0100642562`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9778088465 = 0.9709680952 + 1.000000(0.0100642562)(0.6797075870)`

:math:`w_2 = 0.9778088465 = 0.9709680952 + 1.000000(0.0100642562)(0.6797075870)`

:math:`bias = 0.2686957762 = 0.2586315200 + (1.0000000000)(0.0100642562)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0021274307 = (0.6797075870)(0.3202924130)((0.010064)(0.970968))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3529368698 = 0.3508094391 + 1.000000(0.0021274307)(1.0000000000)`

:math:`w_2 = 0.3529368698 = 0.3508094391 + 1.000000(0.0021274307)(1.0000000000)`

:math:`bias = 0.0529368698 = 0.0508094391 + (1.0000000000)(0.0021274307)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = 0.0021274307 = (0.6797075870)(0.3202924130)((0.010064)(0.970968))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3529368698 = 0.3508094391 + 1.000000(0.0021274307)(1.0000000000)`

:math:`w_2 = 0.3529368698 = 0.3508094391 + 1.000000(0.0021274307)(1.0000000000)`

:math:`bias = 0.0529368698 = 0.0508094391 + (1.0000000000)(0.0021274307)`

Input 2 - Iteration 1
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.3423280259474844, 0.3423280259474844] -> [0.7187275371187261]

Error:  :math:`e = 0.050000 - 0.718728 = -0.6687275371`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.2235982595`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7187275371)(1 - 0.2812724629)(-0.6687275371) = -0.1351887983`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.9315299320 = 0.9778088465 + 1.000000(-0.1351887983)(0.3423280259)`

:math:`w_2 = 0.9315299320 = 0.9778088465 + 1.000000(-0.1351887983)(0.3423280259)`

:math:`bias = 0.1335069779 = 0.2686957762 + (1.0000000000)(-0.1351887983)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0297609274 = (0.3423280259)(0.6576719741)((-0.135189)(0.977809))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3826977972 = 0.3529368698 + 1.000000(-0.0297609274)(-1.0000000000)`

:math:`w_2 = 0.3826977972 = 0.3529368698 + 1.000000(-0.0297609274)(-1.0000000000)`

:math:`bias = 0.0231759423 = 0.0529368698 + (1.0000000000)(-0.0297609274)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0297609274 = (0.3423280259)(0.6576719741)((-0.135189)(0.977809))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.3826977972 = 0.3529368698 + 1.000000(-0.0297609274)(-1.0000000000)`

:math:`w_2 = 0.3826977972 = 0.3529368698 + 1.000000(-0.0297609274)(-1.0000000000)`

:math:`bias = 0.0231759423 = 0.0529368698 + (1.0000000000)(-0.0297609274)`

Input 2 - Iteration 2
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.32251895778640954, 0.32251895778640954] -> [0.6757655001316254]

Error:  :math:`e = 0.050000 - 0.675766 = -0.6257655001`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1957912306`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6757655001)(1 - 0.3242344999)(-0.6257655001) = -0.1371092816`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8873095894 = 0.9315299320 + 1.000000(-0.1371092816)(0.3225189578)`

:math:`w_2 = 0.8873095894 = 0.9315299320 + 1.000000(-0.1371092816)(0.3225189578)`

:math:`bias = -0.0036023038 = 0.1335069779 + (1.0000000000)(-0.1371092816)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0279071871 = (0.3225189578)(0.6774810422)((-0.137109)(0.931530))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4106049843 = 0.3826977972 + 1.000000(-0.0279071871)(-1.0000000000)`

:math:`w_2 = 0.4106049843 = 0.3826977972 + 1.000000(-0.0279071871)(-1.0000000000)`

:math:`bias = -0.0047312448 = 0.0231759423 + (1.0000000000)(-0.0279071871)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0279071871 = (0.3225189578)(0.6774810422)((-0.137109)(0.931530))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4106049843 = 0.3826977972 + 1.000000(-0.0279071871)(-1.0000000000)`

:math:`w_2 = 0.4106049843 = 0.3826977972 + 1.000000(-0.0279071871)(-1.0000000000)`

:math:`bias = -0.0047312448 = 0.0231759423 + (1.0000000000)(-0.0279071871)`

Input 2 - Iteration 3
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.3045039625833064, 0.3045039625833064] -> [0.6310621774250147]

Error:  :math:`e = 0.050000 - 0.631062 = -0.5810621774`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1688166270`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.6310621774)(1 - 0.3689378226)(-0.5810621774) = -0.1352844683`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8461149327 = 0.8873095894 + 1.000000(-0.1352844683)(0.3045039626)`

:math:`w_2 = 0.8461149327 = 0.8873095894 + 1.000000(-0.1352844683)(0.3045039626)`

:math:`bias = -0.1388867721 = -0.0036023038 + (1.0000000000)(-0.1352844683)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0254220590 = (0.3045039626)(0.6954960374)((-0.135284)(0.887310))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4360270433 = 0.4106049843 + 1.000000(-0.0254220590)(-1.0000000000)`

:math:`w_2 = 0.4360270433 = 0.4106049843 + 1.000000(-0.0254220590)(-1.0000000000)`

:math:`bias = -0.0301533038 = -0.0047312448 + (1.0000000000)(-0.0254220590)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0254220590 = (0.3045039626)(0.6954960374)((-0.135284)(0.887310))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4360270433 = 0.4106049843 + 1.000000(-0.0254220590)(-1.0000000000)`

:math:`w_2 = 0.4360270433 = 0.4106049843 + 1.000000(-0.0254220590)(-1.0000000000)`

:math:`bias = -0.0301533038 = -0.0047312448 + (1.0000000000)(-0.0254220590)`

Input 2 - Iteration 4
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.2885970892646716, 0.2885970892646716] -> [0.5864928912665278]

Error:  :math:`e = 0.050000 - 0.586493 = -0.5364928913`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1439123112`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.5864928913)(1 - 0.4135071087)(-0.5364928913) = -0.1301097086`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.8085656495 = 0.8461149327 + 1.000000(-0.1301097086)(0.2885970893)`

:math:`w_2 = 0.8085656495 = 0.8461149327 + 1.000000(-0.1301097086)(0.2885970893)`

:math:`bias = -0.2689964807 = -0.1388867721 + (1.0000000000)(-0.1301097086)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0226019884 = (0.2885970893)(0.7114029107)((-0.130110)(0.846115))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4586290318 = 0.4360270433 + 1.000000(-0.0226019884)(-1.0000000000)`

:math:`w_2 = 0.4586290318 = 0.4360270433 + 1.000000(-0.0226019884)(-1.0000000000)`

:math:`bias = -0.0527552922 = -0.0301533038 + (1.0000000000)(-0.0226019884)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0226019884 = (0.2885970893)(0.7114029107)((-0.130110)(0.846115))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4586290318 = 0.4360270433 + 1.000000(-0.0226019884)(-1.0000000000)`

:math:`w_2 = 0.4586290318 = 0.4360270433 + 1.000000(-0.0226019884)(-1.0000000000)`

:math:`bias = -0.0527552922 = -0.0301533038 + (1.0000000000)(-0.0226019884)`

Input 2 - Iteration 5
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.27487784012226013, 0.27487784012226013] -> [0.54376696914695]

Error:  :math:`e = 0.050000 - 0.543767 = -0.4937669691`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1219029099`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.5437669691)(1 - 0.4562330309)(-0.4937669691) = -0.1224959082`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7748942389 = 0.8085656495 + 1.000000(-0.1224959082)(0.2748778401)`

:math:`w_2 = 0.7748942389 = 0.8085656495 + 1.000000(-0.1224959082)(0.2748778401)`

:math:`bias = -0.3914923889 = -0.2689964807 + (1.0000000000)(-0.1224959082)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0197418467 = (0.2748778401)(0.7251221599)((-0.122496)(0.808566))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4783708785 = 0.4586290318 + 1.000000(-0.0197418467)(-1.0000000000)`

:math:`w_2 = 0.4783708785 = 0.4586290318 + 1.000000(-0.0197418467)(-1.0000000000)`

:math:`bias = -0.0724971390 = -0.0527552922 + (1.0000000000)(-0.0197418467)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0197418467 = (0.2748778401)(0.7251221599)((-0.122496)(0.808566))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4783708785 = 0.4586290318 + 1.000000(-0.0197418467)(-1.0000000000)`

:math:`w_2 = 0.4783708785 = 0.4586290318 + 1.000000(-0.0197418467)(-1.0000000000)`

:math:`bias = -0.0724971390 = -0.0527552922 + (1.0000000000)(-0.0197418467)`

Input 2 - Iteration 6
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.26323168665074126, 0.26323168665074126] -> [0.5041151686020331]

Error:  :math:`e = 0.050000 - 0.504115 = -0.4541151686`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1031102932`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.5041151686)(1 - 0.4958848314)(-0.4541151686) = -0.1135211019`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7450118878 = 0.7748942389 + 1.000000(-0.1135211019)(0.2632316867)`

:math:`w_2 = 0.7450118878 = 0.7748942389 + 1.000000(-0.1135211019)(0.2632316867)`

:math:`bias = -0.5050134907 = -0.3914923889 + (1.0000000000)(-0.1135211019)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0170603578 = (0.2632316867)(0.7367683133)((-0.113521)(0.774894))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4954312364 = 0.4783708785 + 1.000000(-0.0170603578)(-1.0000000000)`

:math:`w_2 = 0.4954312364 = 0.4783708785 + 1.000000(-0.0170603578)(-1.0000000000)`

:math:`bias = -0.0895574968 = -0.0724971390 + (1.0000000000)(-0.0170603578)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0170603578 = (0.2632316867)(0.7367683133)((-0.113521)(0.774894))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.4954312364 = 0.4783708785 + 1.000000(-0.0170603578)(-1.0000000000)`

:math:`w_2 = 0.4954312364 = 0.4783708785 + 1.000000(-0.0170603578)(-1.0000000000)`

:math:`bias = -0.0895574968 = -0.0724971390 + (1.0000000000)(-0.0170603578)`

Input 2 - Iteration 7
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.25342654955402666, 0.25342654955402666] -> [0.46819253462538946]

Error:  :math:`e = 0.050000 - 0.468193 = -0.4181925346`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0874424980`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4681925346)(1 - 0.5318074654)(-0.4181925346) = -0.1041250421`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7186238376 = 0.7450118878 + 1.000000(-0.1041250421)(0.2534265496)`

:math:`w_2 = 0.7186238376 = 0.7450118878 + 1.000000(-0.1041250421)(0.2534265496)`

:math:`bias = -0.6091385328 = -0.5050134907 + (1.0000000000)(-0.1041250421)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0146771943 = (0.2534265496)(0.7465734504)((-0.104125)(0.745012))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5101084307 = 0.4954312364 + 1.000000(-0.0146771943)(-1.0000000000)`

:math:`w_2 = 0.5101084307 = 0.4954312364 + 1.000000(-0.0146771943)(-1.0000000000)`

:math:`bias = -0.1042346912 = -0.0895574968 + (1.0000000000)(-0.0146771943)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0146771943 = (0.2534265496)(0.7465734504)((-0.104125)(0.745012))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5101084307 = 0.4954312364 + 1.000000(-0.0146771943)(-1.0000000000)`

:math:`w_2 = 0.5101084307 = 0.4954312364 + 1.000000(-0.0146771943)(-1.0000000000)`

:math:`bias = -0.1042346912 = -0.0895574968 + (1.0000000000)(-0.0146771943)`

Input 2 - Iteration 8
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.24518650016172638, 0.24518650016172638] -> [0.43616407481578756]

Error:  :math:`e = 0.050000 - 0.436164 = -0.3861640748`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0745613463`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4361640748)(1 - 0.5638359252)(-0.3861640748) = -0.0949673903`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.6953391156 = 0.7186238376 + 1.000000(-0.0949673903)(0.2451865002)`

:math:`w_2 = 0.6953391156 = 0.7186238376 + 1.000000(-0.0949673903)(0.2451865002)`

:math:`bias = -0.7041059231 = -0.6091385328 + (1.0000000000)(-0.0949673903)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0126302613 = (0.2451865002)(0.7548134998)((-0.094967)(0.718624))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5227386920 = 0.5101084307 + 1.000000(-0.0126302613)(-1.0000000000)`

:math:`w_2 = 0.5227386920 = 0.5101084307 + 1.000000(-0.0126302613)(-1.0000000000)`

:math:`bias = -0.1168649525 = -0.1042346912 + (1.0000000000)(-0.0126302613)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0126302613 = (0.2451865002)(0.7548134998)((-0.094967)(0.718624))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5227386920 = 0.5101084307 + 1.000000(-0.0126302613)(-1.0000000000)`

:math:`w_2 = 0.5227386920 = 0.5101084307 + 1.000000(-0.0126302613)(-1.0000000000)`

:math:`bias = -0.1168649525 = -0.1042346912 + (1.0000000000)(-0.0126302613)`

Input 2 - Iteration 9
~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.23824193096867008, 0.23824193096867008] -> [0.4078675011306612]

Error:  :math:`e = 0.050000 - 0.407868 = -0.3578675011`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0640345742`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.4078675011)(1 - 0.5921324989)(-0.3578675011) = -0.0864291537`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.6747480671 = 0.6953391156 + 1.000000(-0.0864291537)(0.2382419310)`

:math:`w_2 = 0.6747480671 = 0.6953391156 + 1.000000(-0.0864291537)(0.2382419310)`

:math:`bias = -0.7905350768 = -0.7041059231 + (1.0000000000)(-0.0864291537)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0109066703 = (0.2382419310)(0.7617580690)((-0.086429)(0.695339))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5336453623 = 0.5227386920 + 1.000000(-0.0109066703)(-1.0000000000)`

:math:`w_2 = 0.5336453623 = 0.5227386920 + 1.000000(-0.0109066703)(-1.0000000000)`

:math:`bias = -0.1277716228 = -0.1168649525 + (1.0000000000)(-0.0109066703)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0109066703 = (0.2382419310)(0.7617580690)((-0.086429)(0.695339))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5336453623 = 0.5227386920 + 1.000000(-0.0109066703)(-1.0000000000)`

:math:`w_2 = 0.5336453623 = 0.5227386920 + 1.000000(-0.0109066703)(-1.0000000000)`

:math:`bias = -0.1277716228 = -0.1168649525 + (1.0000000000)(-0.0109066703)`

Input 2 - Iteration 10
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.23235476182816606, 0.23235476182816606] -> [0.38296710857882077]

Error:  :math:`e = 0.050000 - 0.382967 = -0.3329671086`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0554335477`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.3829671086)(1 - 0.6170328914)(-0.3329671086) = -0.0786812273`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.6564661093 = 0.6747480671 + 1.000000(-0.0786812273)(0.2323547618)`

:math:`w_2 = 0.6564661093 = 0.6747480671 + 1.000000(-0.0786812273)(0.2323547618)`

:math:`bias = -0.8692163042 = -0.7905350768 + (1.0000000000)(-0.0786812273)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0094694534 = (0.2323547618)(0.7676452382)((-0.078681)(0.674748))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5431148157 = 0.5336453623 + 1.000000(-0.0094694534)(-1.0000000000)`

:math:`w_2 = 0.5431148157 = 0.5336453623 + 1.000000(-0.0094694534)(-1.0000000000)`

:math:`bias = -0.1372410762 = -0.1277716228 + (1.0000000000)(-0.0094694534)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0094694534 = (0.2323547618)(0.7676452382)((-0.078681)(0.674748))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5431148157 = 0.5336453623 + 1.000000(-0.0094694534)(-1.0000000000)`

:math:`w_2 = 0.5431148157 = 0.5336453623 + 1.000000(-0.0094694534)(-1.0000000000)`

:math:`bias = -0.1372410762 = -0.1277716228 + (1.0000000000)(-0.0094694534)`

Input 2 - Iteration 11
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.22732624728258596, 0.22732624728258596] -> [0.3610632421453594]

Error:  :math:`e = 0.050000 - 0.361063 = -0.3110632421`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0483801703`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.3610632421)(1 - 0.6389367579)(-0.3110632421) = -0.0717612253`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.6401528992 = 0.6564661093 + 1.000000(-0.0717612253)(0.2273262473)`

:math:`w_2 = 0.6401528992 = 0.6564661093 + 1.000000(-0.0717612253)(0.2273262473)`

:math:`bias = -0.9409775295 = -0.8692163042 + (1.0000000000)(-0.0717612253)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0082746169 = (0.2273262473)(0.7726737527)((-0.071761)(0.656466))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5513894327 = 0.5431148157 + 1.000000(-0.0082746169)(-1.0000000000)`

:math:`w_2 = 0.5513894327 = 0.5431148157 + 1.000000(-0.0082746169)(-1.0000000000)`

:math:`bias = -0.1455156932 = -0.1372410762 + (1.0000000000)(-0.0082746169)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0082746169 = (0.2273262473)(0.7726737527)((-0.071761)(0.656466))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5513894327 = 0.5431148157 + 1.000000(-0.0082746169)(-1.0000000000)`

:math:`w_2 = 0.5513894327 = 0.5431148157 + 1.000000(-0.0082746169)(-1.0000000000)`

:math:`bias = -0.1455156932 = -0.1372410762 + (1.0000000000)(-0.0082746169)`

Input 2 - Iteration 12
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.22299549851628409, 0.22299549851628409] -> [0.3417567989247825]

Error:  :math:`e = 0.050000 - 0.341757 = -0.2917567989`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0425610149`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.3417567989)(1 - 0.6582432011)(-0.2917567989) = -0.0656333438`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.6255169590 = 0.6401528992 + 1.000000(-0.0656333438)(0.2229954985)`

:math:`w_2 = 0.6255169590 = 0.6401528992 + 1.000000(-0.0656333438)(0.2229954985)`

:math:`bias = -1.0066108732 = -0.9409775295 + (1.0000000000)(-0.0656333438)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0072799413 = (0.2229954985)(0.7770045015)((-0.065633)(0.640153))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5586693740 = 0.5513894327 + 1.000000(-0.0072799413)(-1.0000000000)`

:math:`w_2 = 0.5586693740 = 0.5513894327 + 1.000000(-0.0072799413)(-1.0000000000)`

:math:`bias = -0.1527956345 = -0.1455156932 + (1.0000000000)(-0.0072799413)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0072799413 = (0.2229954985)(0.7770045015)((-0.065633)(0.640153))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5586693740 = 0.5513894327 + 1.000000(-0.0072799413)(-1.0000000000)`

:math:`w_2 = 0.5586693740 = 0.5513894327 + 1.000000(-0.0072799413)(-1.0000000000)`

:math:`bias = -0.1527956345 = -0.1455156932 + (1.0000000000)(-0.0072799413)`

Input 2 - Iteration 13
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.2192342488908223, 0.2192342488908223] -> [0.3246811355229012]

Error:  :math:`e = 0.050000 - 0.324681 = -0.2746811355`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0377248631`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.3246811355)(1 - 0.6753188645)(-0.2746811355) = -0.0602274911`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.6123130302 = 0.6255169590 + 1.000000(-0.0602274911)(0.2192342489)`

:math:`w_2 = 0.6123130302 = 0.6255169590 + 1.000000(-0.0602274911)(0.2192342489)`

:math:`bias = -1.0668383643 = -1.0066108732 + (1.0000000000)(-0.0602274911)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0064485640 = (0.2192342489)(0.7807657511)((-0.060227)(0.625517))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5651179380 = 0.5586693740 + 1.000000(-0.0064485640)(-1.0000000000)`

:math:`w_2 = 0.5651179380 = 0.5586693740 + 1.000000(-0.0064485640)(-1.0000000000)`

:math:`bias = -0.1592441985 = -0.1527956345 + (1.0000000000)(-0.0064485640)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0064485640 = (0.2192342489)(0.7807657511)((-0.060227)(0.625517))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5651179380 = 0.5586693740 + 1.000000(-0.0064485640)(-1.0000000000)`

:math:`w_2 = 0.5651179380 = 0.5586693740 + 1.000000(-0.0064485640)(-1.0000000000)`

:math:`bias = -0.1592441985 = -0.1527956345 + (1.0000000000)(-0.0064485640)`

Input 2 - Iteration 14
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.21594082660120595, 0.21594082660120595] -> [0.3095141649020245]

Error:  :math:`e = 0.050000 - 0.309514 = -0.2595141649`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0336738009`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.3095141649)(1 - 0.6904858351)(-0.2595141649) = -0.0554621078`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.6003364968 = 0.6123130302 + 1.000000(-0.0554621078)(0.2159408266)`

:math:`w_2 = 0.6003364968 = 0.6123130302 + 1.000000(-0.0554621078)(0.2159408266)`

:math:`bias = -1.1223004721 = -1.0668383643 + (1.0000000000)(-0.0554621078)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0057498097 = (0.2159408266)(0.7840591734)((-0.055462)(0.612313))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5708677477 = 0.5651179380 + 1.000000(-0.0057498097)(-1.0000000000)`

:math:`w_2 = 0.5708677477 = 0.5651179380 + 1.000000(-0.0057498097)(-1.0000000000)`

:math:`bias = -0.1649940082 = -0.1592441985 + (1.0000000000)(-0.0057498097)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0057498097 = (0.2159408266)(0.7840591734)((-0.055462)(0.612313))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5708677477 = 0.5651179380 + 1.000000(-0.0057498097)(-1.0000000000)`

:math:`w_2 = 0.5708677477 = 0.5651179380 + 1.000000(-0.0057498097)(-1.0000000000)`

:math:`bias = -0.1649940082 = -0.1592441985 + (1.0000000000)(-0.0057498097)`

Input 2 - Iteration 15
~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[-1, -1] -> [0.21303463110300688, 0.21303463110300688] -> [0.2959798617509097]

Error:  :math:`e = 0.050000 - 0.295980 = -0.2459798618`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0302530462`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.2959798618)(1 - 0.7040201382)(-0.2459798618) = -0.0512562463`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.5894171413 = 0.6003364968 + 1.000000(-0.0512562463)(0.2130346311)`

:math:`w_2 = 0.5894171413 = 0.6003364968 + 1.000000(-0.0512562463)(0.2130346311)`

:math:`bias = -1.1735567184 = -1.1223004721 + (1.0000000000)(-0.0512562463)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0051587844 = (0.2130346311)(0.7869653689)((-0.051256)(0.600336))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5760265321 = 0.5708677477 + 1.000000(-0.0051587844)(-1.0000000000)`

:math:`w_2 = 0.5760265321 = 0.5708677477 + 1.000000(-0.0051587844)(-1.0000000000)`

:math:`bias = -0.1701527926 = -0.1649940082 + (1.0000000000)(-0.0051587844)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0051587844 = (0.2130346311)(0.7869653689)((-0.051256)(0.600336))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.5760265321 = 0.5708677477 + 1.000000(-0.0051587844)(-1.0000000000)`

:math:`w_2 = 0.5760265321 = 0.5708677477 + 1.000000(-0.0051587844)(-1.0000000000)`

:math:`bias = -0.1701527926 = -0.1649940082 + (1.0000000000)(-0.0051587844)`


.. _successive-training-final:

Final Network
~~~~~~~~~~~~~~~

.. graphviz::

    digraph network{
    rankdir=LR;
    node [shape=plaintext,width=0.25,label=<x<SUB>1</SUB>>]; x_1;
    node [shape=plaintext,width=0.25,label=<x<SUB>2</SUB>>]; x_2;
    node [shape=square,width=.25,fillcolor=black,style=filled,label=""]; input_0, input_1
    node [shape=circle,width=.5,fillcolor=white,label="-0.170153"]; h_0_0;
    node [shape=circle,width=.5,fillcolor=white,label="-0.170153"]; h_0_1;
    node [shape=circle,width=.5,fillcolor=white,label="-1.173557"]; o_0;
    node [shape=circle,width=.5,style=invisible,label=""]; output;
    
    
    x_1 -> input_0;
    x_2 -> input_1;
    input_0 -> h_0_0 [label="0.576027"];
    input_1 -> h_0_0 [label="0.576027"];
    input_0 -> h_0_1 [label="0.576027"];
    input_1 -> h_0_1 [label="0.576027"];
    h_0_0 -> o_0 [label="0.589417"];
    h_0_1 -> o_0 [label="0.589417"];
    o_0 -> output [label="Output"];
    }
    


.. _successive-validation-test:

Successive Training Validation Test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

[1, 1] -> [0.7274851090012344, 0.7274851090012344] -> [0.4216576338099176]

Error:  :math:`e = 0.900000 - 0.421658 = 0.4783423662`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.1144057096`

0.4216576338099176

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

[-1, -1] -> [0.21045153050076515, 0.21045153050076515] -> [0.2838448109486975]

Error:  :math:`e = 0.050000 - 0.283845 = -0.2338448109`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0273416978`

0.2838448109486975


Analysis Writeup
----------------

After running through the two training methods, it can be observed from comparing the
:ref:`alternating-validation-test` and :ref:`successive-validation-test` that the
Alternating training method (Method 1) resulted in a smaller error after 15 iterations of
training for the first input, but the successive method resulted in a smaller error for the
second input.

Overall, it appears that the alternating method was able to minimize errors across both inputs.
The largest Mean Squared Error for Method 1 was 0.0550343269 which is still less than the largest
Mean Squared Error for Method 2, 0.1144057096.

When looking at the results of the successive method, we see that the error for the first input
becomes very small by the 15th iteration. However, immediately after when it starts training the
second input, the error is significant. so it must undo some of the training done in the previous
15 iterations. Adjusting the initial weights would affect how far the optimization process would
go in the direction of training the Neural Network for the first input.


:ref:`successive-training-final`.
:ref:`alternating-training-final` 


