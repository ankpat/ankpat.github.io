
Homework 5
==========

.. contents:: Table of Contents
    :depth: 2

For this assignment, create a neural network program using a language of your choice that
implements the feedforward, back propagation algorithm for a network with the following topology
like the one in the lecture:

.. graphviz::

    digraph simple_network {
        rankdir=LR;

        node [shape=plaintext,width=0.25,label=<x<SUB>1</SUB>>]; x_1;
        node [shape=plaintext,width=0.25,label=<x<SUB>2</SUB>>]; x_2;
        node [shape=square,width=.25,fillcolor=black,style=filled,label=""]; input_0, input_1;
        node [shape=circle,width=.5,fillcolor=white,label=""]; h_0_0, h_0_1;
        node [shape=circle,width=.5,fillcolor=white,label=""]; o_0;
        node [shape=circle,width=.5,style=invisible,label=""]; output;


        x_1 -> input_0;
        x_2 -> input_1;
        input_0 -> h_0_0;
        input_1 -> h_0_0;

        input_0 -> h_0_1;
        input_1 -> h_0_1;

        h_0_0 -> o_0;
        h_0_1 -> o_0;

        o_0 -> output [label="Output"];
    }




Testing
-------


Use the example from class to verify that the network performs correctly.

The initial weights for each node of the hidden layer are 0.3 for all inputs and the initial
weights for all inputs to the output layer are 0.8.

The Gain factor :math:`\eta = 1.0` and all biases are set to 0.

The Input {1, 2} should generate a desired output of 0.7.

Biases will not be updated for this run of the training algorithm.



Initial Network
~~~~~~~~~~~~~~~

.. graphviz::

    digraph network{
    rankdir=LR;
    node [shape=plaintext,width=0.25,label=<x<SUB>1</SUB>>]; x_1;
    node [shape=plaintext,width=0.25,label=<x<SUB>2</SUB>>]; x_2;
    node [shape=square,width=.25,fillcolor=black,style=filled,label=""]; input_0, input_1
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; h_0_0;
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; h_0_1;
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; o_0;
    node [shape=circle,width=.5,style=invisible,label=""]; output;
    
    
    x_1 -> input_0;
    x_2 -> input_1;
    input_0 -> h_0_0 [label="0.300000"];
    input_1 -> h_0_0 [label="0.300000"];
    input_0 -> h_0_1 [label="0.300000"];
    input_1 -> h_0_1 [label="0.300000"];
    h_0_0 -> o_0 [label="0.800000"];
    h_0_1 -> o_0 [label="0.800000"];
    o_0 -> output [label="Output"];
    }
    

Iteration 1
~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

Output of each layer:

[1, 2] -> [0.7109495026250039, 0.7109495026250039] -> [0.757223870792493]

Error:  :math:`e = 0.700000 - 0.757224 = -0.0572238708`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0016372857`

Back-Propagation Phase
++++++++++++++++++++++

Output Layer Delta
<<<<<<<<<<<<<<<<<<

:math:`\delta = (0.7572238708)(1 - 0.2427761292)(-0.0572238708) = -0.0105198007`

Output Layer Weights
<<<<<<<<<<<<<<<<<<<<

:math:`w_1 = 0.7925209530 = 0.8000000000 + 1.000000(-0.0105198007)(0.7109495026)`

:math:`w_2 = 0.7925209530 = 0.8000000000 + 1.000000(-0.0105198007)(0.7109495026)`

Hidden Layer
<<<<<<<<<<<<

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0017294578 = (0.7109495026)(0.2890504974)((-0.010520)(0.800000))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.2982705422 = 0.3000000000 + 1.000000(-0.0017294578)(1.0000000000)`

:math:`w_2 = 0.2965410844 = 0.3000000000 + 1.000000(-0.0017294578)(2.0000000000)`

Hidden Layer Delta
>>>>>>>>>>>>>>>>>>

:math:`\delta = -0.0017294578 = (0.7109495026)(0.2890504974)((-0.010520)(0.800000))`

Hidden Layer Weights
>>>>>>>>>>>>>>>>>>>>

:math:`w_1 = 0.2982705422 = 0.3000000000 + 1.000000(-0.0017294578)(1.0000000000)`

:math:`w_2 = 0.2965410844 = 0.3000000000 + 1.000000(-0.0017294578)(2.0000000000)`

.. graphviz::

    digraph network{
    rankdir=LR;
    node [shape=plaintext,width=0.25,label=<x<SUB>1</SUB>>]; x_1;
    node [shape=plaintext,width=0.25,label=<x<SUB>2</SUB>>]; x_2;
    node [shape=square,width=.25,fillcolor=black,style=filled,label=""]; input_0, input_1
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; h_0_0;
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; h_0_1;
    node [shape=circle,width=.5,fillcolor=white,label="0.000000"]; o_0;
    node [shape=circle,width=.5,style=invisible,label=""]; output;
    
    
    x_1 -> input_0;
    x_2 -> input_1;
    input_0 -> h_0_0 [label="0.298271"];
    input_1 -> h_0_0 [label="0.296541"];
    input_0 -> h_0_1 [label="0.298271"];
    input_1 -> h_0_1 [label="0.296541"];
    h_0_0 -> o_0 [label="0.792521"];
    h_0_1 -> o_0 [label="0.792521"];
    o_0 -> output [label="Output"];
    }
    


Validation Test
~~~~~~~~~~~~~~~

Feed Forward Epoch
^^^^^^^^^^^^^^^^^^

[1, 2] -> [0.7091692457152926, 0.7091692457152926] -> [0.7547415782405359]

Error:  :math:`e = 0.700000 - 0.754742 = -0.0547415782`

Error squared  :math:`E = \frac{1}{2}e^2 = 0.0014983202`

0.7547415782405359

Alternating Training Method
---------------------------


For each cycle of this training procedure, present the first input/output pair,
perform the back propagation technique to update the weights, then present the second
input/output pair and again perform the back propagation technique to update the
weights. This constitutes a single cycle.

These are the results of executing 15 iterations of training with this method with the following
inputs:

.. math:: ( 1, 1) \rightarrow 0.9

.. math:: (-1, -1) \rightarrow 0.05



Successive Training Method
--------------------------


For each input/output pair in the training set, present the pair and perform the back propagation
technique to update the weights for the specified number of iterations. Then move onto the next
input/output pair and repeat until no more training data is available.

These are the results of executing 15 iterations of training with this method with the following
inputs:

.. math:: ( 1, 1) \rightarrow 0.9

.. math:: (-1, -1) \rightarrow 0.05


.. contents:: Iterations
    :depth: 2
    :local:


Analysis Writeup
----------------

After running through the two training methods...


