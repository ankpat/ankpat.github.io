Analysis Writeup
----------------

**How do the two methods compare?  Is one method superior to the other?**

Note: When referring to Error below, this specifically refers to the sum squared error :math:`E = \frac{1}{2}(d - y)^2` where *d* is the desired output and *y* is the actual output.

After running through the two training methods, it can be observed from comparing the
:ref:`alternating-validation-test` and :ref:`successive-validation-test` that the
Alternating training method (Method 1) resulted in a smaller error after 15 iterations of
training for the first input, but the successive method resulted in a smaller error for the
second input.

Overall, it appears that the alternating method was able to minimize errors across both inputs more evenly than Method 2.
The largest Error for Method 1 was 0.0550343269 which is still less than the largest Error for Method 2, 0.1144057096.
The smallest error was produced by Method 2 (0.0273416978), which is less than the smallest error produced by Method 1.

With just this limited test, it is hard to say if one method is superior to the other. What can be observed in this small experiment is that Method 1
appears to minimize errors across all inputs better than Method 2. Intuitively, Method 1 seems as though it would perform better overall as it is less
likely to be pulled away to a local minimum for one input and then get stuck there even though another, lower minimum is available. The downside to this
is that if the initial weights are such that the training quickly converges to a local minimum, it might end up getting "stuck" in a non-optimal minimum.

Method 2 can overcome such a problem because it trains on one set of input/output pairs at a time. The downside to this approach is that the weights may
minimize the error for one input, but that potentially force the error to be very high for another input. Then, as Method 2 trains on future inputs, it
no longer considers the impact of previous inputs/output pairs. The end result of this can be seen in :ref:`successive-validation-test` where the smallest error is very small, but the largest error is much larger than the largest from Method 1.

**What do you think might happen if we approached the problem in the same way as above, but with different initial values for the weights?**

Adjusting the initial weights would affect how far the optimization process would go in the direction of training the Neural Network for the first input.
When looking at the results of the successive method, we see that the error for the first input becomes very small by the 15th iteration (:ref:`successive-training-input-1-15-rounds`).
However, immediately after when it starts training the second input, the error is significant for the new input, so it must undo some of the training done in the previous 15 iterations.
With different initial weights, it is possible that the weights after 15 iterations would be such that the error for the second input could be minimized further than it could with the statically assigned initial values.

By examining the weights and biases of the two final networks (:ref:`successive-training-final`) (:ref:`alternating-training-final`), it can be observed that both methods have preserved symmetry of the weights and biases. For some situations, it is possible that the optimal solution may be one with symmetrical weights and biases, however, it is also possible that there may be better solutions which minimize errors across the board with asymmetrical weights and biases for individual perceptrons.

