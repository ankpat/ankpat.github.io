
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>NeuralNetwork.Network &#8212; Neural Network 1.0 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for NeuralNetwork.Network</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="sd">&quot;&quot;&quot;Neural Network Implementation</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">.Types</span> <span class="k">import</span> <span class="n">INPUTS</span>

<span class="kn">from</span> <span class="nn">.Layer</span> <span class="k">import</span> <span class="n">Layer</span>

<span class="n">LAYERS</span> <span class="o">=</span> <span class="n">List</span><span class="p">[</span><span class="n">Layer</span><span class="p">]</span>

<div class="viewcode-block" id="Network"><a class="viewcode-back" href="../../NeuralNetwork.html#NeuralNetwork.Network.Network">[docs]</a><span class="k">class</span> <span class="nc">Network</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A Neural Network consists of one or more Layer objects containing Perceptrons. A Neural</span>
<span class="sd">    Network receives one or more inputs and produces a single output.</span>
<span class="sd">    This implementation can be trained using the Feed-Forward/Back-Propagation method which</span>
<span class="sd">    attempts to find the global minimum of the Mean Squared Error.</span>

<span class="sd">    Mathematically, this Neural Network is a mapping :math:`\mathbb{R}^n \rightarrow \mathbb{R}`.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">:</span> <span class="n">LAYERS</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create a new Feed Forward Neural Network</span>

<span class="sd">        Arguments:</span>
<span class="sd">            layers (Layers): A list of layer objects</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span>

    <span class="k">def</span> <span class="nf">_feed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">INPUTS</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;Internal feed method that returns the output of each layer of the network.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            inputs (INPUTS): A list of inputs given to this Neural Network</span>

<span class="sd">        Returns:</span>
<span class="sd">            List[List[float]]: The output of each layer of the Neural Network</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">results</span>

<div class="viewcode-block" id="Network.feed"><a class="viewcode-back" href="../../NeuralNetwork.html#NeuralNetwork.Network.Network.feed">[docs]</a>    <span class="k">def</span> <span class="nf">feed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">INPUTS</span><span class="p">,</span> <span class="n">desired</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Feed inputs to the network and return single output.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            inputs (INPUTS): A list of inputs given to this Neural Network</span>

<span class="sd">        Returns:</span>
<span class="sd">            float: The final value returned by the output layer.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feed</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">desired</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Feed Forward Epoch&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;^^^^^^^^^^^^^^^^^^</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; -&gt; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">results</span><span class="p">)))</span>
            <span class="nb">print</span><span class="p">()</span>

            <span class="n">error</span> <span class="o">=</span> <span class="n">desired</span> <span class="o">-</span> <span class="n">output</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: &quot;</span><span class="p">,</span> <span class="s2">&quot;:math:`e = </span><span class="si">%f</span><span class="s2"> - </span><span class="si">%f</span><span class="s2"> = </span><span class="si">%.10f</span><span class="s2">`&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">desired</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">error</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error squared &quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;:math:`E = \frac</span><span class="si">{1}{2}</span><span class="s2">e^2 = </span><span class="si">%.10f</span><span class="s2">`&quot;</span><span class="o">%</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">error</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">output</span></div>

<div class="viewcode-block" id="Network.train_round"><a class="viewcode-back" href="../../NeuralNetwork.html#NeuralNetwork.Network.Network.train_round">[docs]</a>    <span class="k">def</span> <span class="nf">train_round</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">inputs</span><span class="p">:</span> <span class="n">INPUTS</span><span class="p">,</span>
                    <span class="n">desired</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                    <span class="n">gain_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                    <span class="n">train_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Perform a single round of the Feed-Forward/Back-Propagation (FFBP) algorithm to train</span>
<span class="sd">        a Neural Network.</span>

<span class="sd">        The FFBP algorithm as two phases as the name implies.</span>

<span class="sd">        #) In the Feed-Forward phase, the inputs are fed through the Neural Network as is</span>
<span class="sd">           and the final output value is used to compute the error and the squared error.</span>

<span class="sd">           The Neural Network maps :math:`\mathbb{R}^n \rightarrow \mathbb{R}`. The input is a</span>
<span class="sd">           vector of real numbers and the output is a single real number. The difference between</span>
<span class="sd">           the actual output value and the expected output for a given set of inputs is the Error</span>
<span class="sd">           :math:`e`. The Mean Square Error, :math:`E = \frac{1}{2} e^2` is minimized using the</span>
<span class="sd">           Perceptron Delta Rule.</span>


<span class="sd">        #) In the Back-Propagation phase, the error values are then used to calculate changes in</span>
<span class="sd">           the weights for each layer of the Neural Network.</span>

<span class="sd">           This delta value is then propagated back through the Neural Network&#39;s hidden layers to</span>
<span class="sd">           update the weights of hidden layer perceptrons. Deltas are propagated backwards through</span>
<span class="sd">           the Neural Network until the weights of the first layer have been updated.</span>

<span class="sd">           Then the training process can either continue for another round or can terminate.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            inputs (INPUTS): A list of input values.</span>
<span class="sd">            desired (float): The expected output value of this Neural Network for the given input.</span>
<span class="sd">            gain_factor (float): The factor by which each successive weight</span>
<span class="sd">                                 term is shifted each iteration.</span>
<span class="sd">            train_bias (bool): Flag to incorporate training of the Bias.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Feed Forward Epoch&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;^^^^^^^^^^^^^^^^^^</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">old_weights</span> <span class="o">=</span> <span class="p">[[</span><span class="n">n</span><span class="o">.</span><span class="n">weights</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>
        <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feed</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output of each layer:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; -&gt; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">results</span><span class="p">))</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">error</span> <span class="o">=</span> <span class="n">desired</span> <span class="o">-</span> <span class="n">output</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: &quot;</span><span class="p">,</span> <span class="s2">&quot;:math:`e = </span><span class="si">%f</span><span class="s2"> - </span><span class="si">%f</span><span class="s2"> = </span><span class="si">%.10f</span><span class="s2">`&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">desired</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">error</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error squared &quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;:math:`E = \frac</span><span class="si">{1}{2}</span><span class="s2">e^2 = </span><span class="si">%.10f</span><span class="s2">`&quot;</span><span class="o">%</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">error</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">()</span>

        <span class="n">delta</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">output</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">error</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Back-Propagation Phase&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;++++++++++++++++++++++</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output Layer Delta&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;:math:`\delta = (</span><span class="si">%.10f</span><span class="s2">)(1 - </span><span class="si">%.10f</span><span class="s2">)(</span><span class="si">%.10f</span><span class="s2">) = </span><span class="si">%.10f</span><span class="s2">`&quot;</span><span class="o">%</span>\
                <span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">output</span><span class="p">,</span> <span class="n">error</span><span class="p">,</span> <span class="n">delta</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">()</span>

        <span class="n">layer_deltas</span> <span class="o">=</span> <span class="p">[[</span><span class="n">delta</span><span class="p">]]</span>

        <span class="c1"># Include the input layer in case there is only one layer in this network</span>
        <span class="n">aggregate_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">results</span>
        <span class="n">weight_adjustments</span> <span class="o">=</span> <span class="p">[</span><span class="n">gain_factor</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aggregate_results</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
        <span class="n">weight_adjustments_str</span> <span class="o">=</span> <span class="p">[(</span><span class="n">gain_factor</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">aggregate_results</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>

        <span class="c1"># There is only one node in the output layer</span>
        <span class="n">output_node</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># If append another delta if the bias is subject to training</span>
        <span class="k">if</span> <span class="n">train_bias</span><span class="p">:</span>
            <span class="n">weight_adjustments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gain_factor</span> <span class="o">*</span> <span class="n">delta</span><span class="p">)</span>
            <span class="n">old_bias</span> <span class="o">=</span> <span class="n">output_node</span><span class="o">.</span><span class="n">bias</span>

        <span class="c1"># Adjust output node&#39;s weights</span>
        <span class="n">output_node</span><span class="o">.</span><span class="n">adjust_weights</span><span class="p">(</span><span class="n">weight_adjustments</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output Layer Weights&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">zip_iter</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">output_node</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
                       <span class="n">old_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                       <span class="n">weight_adjustments_str</span><span class="p">,</span>
                       <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_node</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">old_weight</span><span class="p">,</span> <span class="n">wat</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">zip_iter</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;:math:`w_</span><span class="si">%d</span><span class="s2"> = </span><span class="si">%.10f</span><span class="s2"> = </span><span class="si">%.10f</span><span class="s2"> + </span><span class="si">%f</span><span class="s2">(</span><span class="si">%.10f</span><span class="s2">)(</span><span class="si">%.10f</span><span class="s2">)`</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">old_weight</span><span class="p">,</span> <span class="o">*</span><span class="n">wat</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">train_bias</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;:math:`bias = </span><span class="si">%.10f</span><span class="s2"> = </span><span class="si">%.10f</span><span class="s2"> + (</span><span class="si">%.10f</span><span class="s2">)(</span><span class="si">%.10f</span><span class="s2">)`</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">%</span>\
                    <span class="p">(</span><span class="n">output_node</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">old_bias</span><span class="p">,</span> <span class="n">gain_factor</span><span class="p">,</span> <span class="n">delta</span><span class="p">))</span>

        <span class="c1"># In Back-Propagation, start at the second to last layer</span>
        <span class="c1"># as the output layer has already been updated above</span>
        <span class="k">for</span> <span class="n">layer_index</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>

            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hidden Layer&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_index</span><span class="p">]</span>

            <span class="c1"># Keep track of the deltas from this layer as they are combined for layers</span>
            <span class="c1"># earlier in the Neural Network</span>
            <span class="n">this_layer_deltas</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># Iterate over each Perceptron in the layer</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="p">)):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hidden Layer Delta&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="c1"># For each Perceptron in the layer above this one</span>
                <span class="n">delta_str</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_index</span><span class="o">+</span><span class="mi">1</span><span class="p">])):</span>
                    <span class="c1"># Aggregate all the deltas</span>
                    <span class="n">delta</span> <span class="o">+=</span> <span class="n">layer_deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">old_weights</span><span class="p">[</span><span class="n">layer_index</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>

                <span class="n">delta_str</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;(</span><span class="si">%f</span><span class="s2">)(</span><span class="si">%f</span><span class="s2">)&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">layer_deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">k</span><span class="p">],</span> <span class="n">old_weights</span><span class="p">[</span><span class="n">layer_index</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>\
                                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_index</span><span class="o">+</span><span class="mi">1</span><span class="p">]))]</span>

                <span class="c1"># Compute the final delta value for this Perceptron</span>
                <span class="n">delta</span> <span class="o">*=</span> <span class="n">results</span><span class="p">[</span><span class="n">layer_index</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">results</span><span class="p">[</span><span class="n">layer_index</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>

                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;:math:`</span><span class="se">\\</span><span class="s2">delta = </span><span class="si">%.10f</span><span class="s2"> = (</span><span class="si">%.10f</span><span class="s2">)(</span><span class="si">%.10f</span><span class="s2">)(</span><span class="si">%s</span><span class="s2">)`</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span>
                    <span class="n">delta</span><span class="p">,</span>
                    <span class="n">results</span><span class="p">[</span><span class="n">layer_index</span><span class="p">][</span><span class="n">j</span><span class="p">],</span>
                    <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">results</span><span class="p">[</span><span class="n">layer_index</span><span class="p">][</span><span class="n">j</span><span class="p">]),</span> <span class="s1">&#39; + &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">delta_str</span><span class="p">)))</span>
                <span class="c1"># Keep track of this Perceptron&#39;s delta for use in future layers</span>
                <span class="n">this_layer_deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>

                <span class="c1"># Depending on which layer this is, use the results of the previous layer</span>
                <span class="c1"># or the input values for computing the deltas for each of the weights</span>
                <span class="c1"># of the Perceptron</span>
                <span class="k">if</span> <span class="n">layer_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">adjust</span> <span class="o">=</span> <span class="p">[</span><span class="n">gain_factor</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
                    <span class="n">adjust_str</span> <span class="o">=</span> <span class="p">[(</span><span class="n">gain_factor</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">adjust</span> <span class="o">=</span> <span class="p">[</span><span class="n">gain_factor</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="n">layer_index</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
                    <span class="n">adjust_str</span> <span class="o">=</span> <span class="p">[(</span><span class="n">gain_factor</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="n">layer_index</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>

                <span class="c1"># If append another delta if the bias is subject to training</span>
                <span class="k">if</span> <span class="n">train_bias</span><span class="p">:</span>
                    <span class="n">adjust</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gain_factor</span> <span class="o">*</span> <span class="n">delta</span><span class="p">)</span>
                    <span class="n">old_bias</span> <span class="o">=</span> <span class="n">layer</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span>

                <span class="c1"># Provide all deltas to Perceptron so it can update its weights</span>
                <span class="n">layer</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">adjust_weights</span><span class="p">(</span><span class="n">adjust</span><span class="p">)</span>

                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hidden Layer Weights&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="n">zip_iter</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
                               <span class="n">old_weights</span><span class="p">[</span><span class="n">layer_index</span><span class="p">][</span><span class="n">j</span><span class="p">],</span>
                               <span class="n">adjust_str</span><span class="p">,</span>
                               <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">old_weight</span><span class="p">,</span> <span class="n">wat</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">zip_iter</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;:math:`w_</span><span class="si">%d</span><span class="s2"> = </span><span class="si">%.10f</span><span class="s2"> = </span><span class="si">%.10f</span><span class="s2"> + </span><span class="si">%f</span><span class="s2">(</span><span class="si">%.10f</span><span class="s2">)(</span><span class="si">%.10f</span><span class="s2">)`</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">%</span>\
                            <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">old_weight</span><span class="p">,</span> <span class="o">*</span><span class="n">wat</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">train_bias</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;:math:`bias = </span><span class="si">%.10f</span><span class="s2"> = </span><span class="si">%.10f</span><span class="s2"> + (</span><span class="si">%.10f</span><span class="s2">)(</span><span class="si">%.10f</span><span class="s2">)`</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">%</span>\
                            <span class="p">(</span><span class="n">layer</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">old_bias</span><span class="p">,</span> <span class="n">gain_factor</span><span class="p">,</span> <span class="n">delta</span><span class="p">))</span>


            <span class="c1"># Add all deltas from this layer for future layers to use</span>
            <span class="n">layer_deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">this_layer_deltas</span><span class="p">)</span></div>

<div class="viewcode-block" id="Network.train_successive"><a class="viewcode-back" href="../../NeuralNetwork.html#NeuralNetwork.Network.Network.train_successive">[docs]</a>    <span class="k">def</span> <span class="nf">train_successive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">INPUTS</span><span class="p">],</span>
                         <span class="n">desired</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                         <span class="n">gain_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                         <span class="n">iterations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">train_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Train a neural network with multiple input/output pairs.</span>

<span class="sd">        Successive training means that the input/output pairs will be used</span>
<span class="sd">        one at a time to train the neural network for the specified number of iterations</span>
<span class="sd">        before moving on to the next input/output pair.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            inputs (List[INPUTS]): A list of training data for the Neural Network</span>
<span class="sd">            desired (List[float]): The expected outputs for each input set</span>
<span class="sd">            gain_factor (float): The factor by which each successive weight</span>
<span class="sd">                                 term is shifted each iteration.</span>
<span class="sd">            iterations (int): The number of iterations of training to perform for each input</span>
<span class="sd">            train_bias (bool): Flag to incorporate training of the Bias. Disabled by default.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">input_index</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">desired</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">header</span> <span class="o">=</span> <span class="s2">&quot;Input </span><span class="si">%d</span><span class="s2"> - Iteration </span><span class="si">%d</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">input_index</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">header</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;~&quot;</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">header</span><span class="p">))</span>
                <span class="nb">print</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_round</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">gain_factor</span><span class="p">,</span> <span class="n">train_bias</span><span class="p">)</span></div>

<div class="viewcode-block" id="Network.train_alternating"><a class="viewcode-back" href="../../NeuralNetwork.html#NeuralNetwork.Network.Network.train_alternating">[docs]</a>    <span class="k">def</span> <span class="nf">train_alternating</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">INPUTS</span><span class="p">],</span>
                          <span class="n">desired</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                          <span class="n">gain_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                          <span class="n">iterations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                          <span class="n">train_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Train a neural network with multiple input/output pairs.</span>

<span class="sd">        Alternating training means that the input/output pairs will be used one after the other</span>
<span class="sd">        for the specified number of iterations. Given a set of inputs (I1, I2, I3...) and</span>
<span class="sd">        outputs (O1, O2, O3), the Neural Network will be trained on (I1, O1), (I2,O2), (I3, O3)</span>
<span class="sd">        for every of the specified iterations.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            inputs (List[INPUTS]): A list of training data for the Neural Network</span>
<span class="sd">            desired (List[float]): The expected outputs for each input set</span>
<span class="sd">            gain_factor (float): The factor by which each successive weight</span>
<span class="sd">                                 term is shifted each iteration.</span>
<span class="sd">            iterations (int): The number of iterations of training to perform</span>
<span class="sd">            train_bias (bool): Flag to incorporate training of the Bias. Disabled by default.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">last_last</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">last_result</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_data</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration </span><span class="si">%d</span><span class="s2">&quot;</span><span class="o">%</span><span class="n">i</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;~&quot;</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;Iteration </span><span class="si">%d</span><span class="s2">&quot;</span><span class="o">%</span><span class="n">i</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">expected</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">desired</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_round</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">gain_factor</span><span class="p">,</span> <span class="n">train_bias</span><span class="p">)</span>
                <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_data</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">res</span> <span class="o">==</span> <span class="n">last_result</span> <span class="ow">and</span> <span class="n">last_result</span> <span class="o">==</span> <span class="n">last_last</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">last_last</span> <span class="o">=</span> <span class="n">last_result</span>
            <span class="n">last_result</span> <span class="o">=</span> <span class="n">res</span></div>


<div class="viewcode-block" id="Network.train"><a class="viewcode-back" href="../../NeuralNetwork.html#NeuralNetwork.Network.Network.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">inputs</span><span class="p">:</span> <span class="n">INPUTS</span><span class="p">,</span>
              <span class="n">desired</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
              <span class="n">gain_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
              <span class="n">iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
              <span class="n">train_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Train a neural network with a single Input/Ouput pair</span>

<span class="sd">        Arguments:</span>
<span class="sd">            inputs (INPUTS): Training data for the Neural Network</span>
<span class="sd">            desired (float): The expected output</span>
<span class="sd">            gain_factor (float): The factor by which each successive weight</span>
<span class="sd">                                 term is shifted each iteration.</span>
<span class="sd">            iterations (int): The number of iterations of training to perform</span>
<span class="sd">            train_bias (bool): Flag to incorporate training of the Bias. Disabled by default.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">last_last</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">last_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Iteration </span><span class="si">%d</span><span class="s2">&quot;</span><span class="o">%</span><span class="n">i</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;~&quot;</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;Iteration </span><span class="si">%d</span><span class="s2">&quot;</span><span class="o">%</span><span class="n">i</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_round</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">desired</span><span class="p">,</span> <span class="n">gain_factor</span><span class="p">,</span> <span class="n">train_bias</span><span class="o">=</span><span class="n">train_bias</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">res</span> <span class="o">==</span> <span class="n">last_result</span> <span class="ow">and</span> <span class="n">last_result</span> <span class="o">==</span> <span class="n">last_last</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">last_last</span> <span class="o">=</span> <span class="n">last_result</span>
            <span class="n">last_result</span> <span class="o">=</span> <span class="n">res</span></div>

    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;[&quot;</span> <span class="o">+</span> <span class="s1">&#39; -&gt; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">+</span><span class="s2">&quot;]&quot;</span>

<div class="viewcode-block" id="Network.to_dot"><a class="viewcode-back" href="../../NeuralNetwork.html#NeuralNetwork.Network.Network.to_dot">[docs]</a>    <span class="k">def</span> <span class="nf">to_dot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return a string representation of this Neural Network in graphviz DOT format</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: DOT representation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;digraph network{</span><span class="se">\n</span><span class="s2">rankdir=LR;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="c1"># Get number of inputs:</span>
        <span class="n">num_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">number_of_inputs</span>
        <span class="n">input_txt_fmt</span> <span class="o">=</span> <span class="s2">&quot;node [shape=plaintext,width=0.25,label=&lt;x&lt;SUB&gt;</span><span class="si">%d</span><span class="s2">&lt;/SUB&gt;&gt;]; x_</span><span class="si">%d</span><span class="s2">;&quot;</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_txt_fmt</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_inputs</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>

        <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;node [shape=square,width=.25,fillcolor=black,style=filled,label=&quot;&quot;]; &#39;</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;input_</span><span class="si">%d</span><span class="s2">&quot;</span><span class="o">%</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">))</span>

        <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="c1"># Create nodes for hidden layer perceptrons</span>
        <span class="n">hidden_node</span> <span class="o">=</span> <span class="s1">&#39;node [shape=circle,width=.5,fillcolor=white,label=&quot;</span><span class="si">%f</span><span class="s1">&quot;]; h_</span><span class="si">%d</span><span class="s1">_</span><span class="si">%d</span><span class="s1">;</span><span class="se">\n</span><span class="s1">&#39;</span>

        <span class="k">for</span> <span class="n">layer_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_index</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="p">)):</span>
                <span class="n">s</span> <span class="o">+=</span> <span class="n">hidden_node</span><span class="o">%</span><span class="p">(</span><span class="n">layer</span><span class="p">[</span><span class="n">p</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

        <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;node [shape=circle,width=.5,fillcolor=white,label=&quot;</span><span class="si">%f</span><span class="s1">&quot;]; o_0;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">%</span>\
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;node [shape=circle,width=.5,style=invisible,label=&quot;&quot;]; output;</span><span class="se">\n</span><span class="s1">&#39;</span>

        <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>

        <span class="c1"># Input labels to all input nodes</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_inputs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;x_</span><span class="si">%d</span><span class="s2"> -&gt; input_</span><span class="si">%d</span><span class="s2">;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


        <span class="c1"># Inputs of one layer to the next</span>
        <span class="k">for</span> <span class="n">layer_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_index</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="p">)):</span>
                <span class="c1"># for every node in this layer, draw input from all previous nodes</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="n">layer</span><span class="p">[</span><span class="n">p</span><span class="p">]</span><span class="o">.</span><span class="n">weights</span>
                <span class="k">if</span> <span class="n">layer_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># Use inputs</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">number_of_inputs</span><span class="p">):</span>
                        <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;input_</span><span class="si">%d</span><span class="s1"> -&gt; h_</span><span class="si">%d</span><span class="s1">_</span><span class="si">%d</span><span class="s1"> [label=&quot;</span><span class="si">%f</span><span class="s1">&quot;];</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">number_of_inputs</span><span class="p">):</span>
                        <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;h_</span><span class="si">%d</span><span class="s1">_</span><span class="si">%d</span><span class="s1"> -&gt; h_</span><span class="si">%d</span><span class="s1">_</span><span class="si">%d</span><span class="s1"> [label=&quot;</span><span class="si">%f</span><span class="s1">&quot;];</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">%</span>\
                                <span class="p">(</span><span class="n">layer_index</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>


        <span class="c1"># All final layer to the output node</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">last_layer_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">last_layer_index</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="p">)):</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;h_</span><span class="si">%d</span><span class="s1">_</span><span class="si">%d</span><span class="s1"> -&gt; o_0 [label=&quot;</span><span class="si">%f</span><span class="s1">&quot;];</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">last_layer_index</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">p</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># There&#39;s only a single layer that&#39;s the output layer?</span>
            <span class="k">pass</span>

        <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;o_0 -&gt; output [label=&quot;Output&quot;];</span><span class="se">\n</span><span class="s1">&#39;</span>

        <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;}</span><span class="se">\n</span><span class="s2">&quot;</span>

        <span class="k">return</span> <span class="n">s</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Neural Network</a></h1>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=ankpat&repo=jhu&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../homework5.html">Homework 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../NeuralNetwork.html">NeuralNetwork Package</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ankit Patel.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>