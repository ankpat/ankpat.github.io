
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Analysis Writeup &#8212; Neural Network 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="analysis-writeup">
<h1>Analysis Writeup<a class="headerlink" href="#analysis-writeup" title="Permalink to this headline">¶</a></h1>
<p><strong>How do the two methods compare?  Is one method superior to the other?</strong></p>
<p>Note: When referring to Error below, this specifically refers to the sum squared error <span class="math">\(E = \frac{1}{2}(d - y)^2\)</span> where <em>d</em> is the desired output and <em>y</em> is the actual output.</p>
<p>After running through the two training methods, it can be observed from comparing the
<a class="reference internal" href="homework5.html#alternating-validation-test"><span class="std std-ref">Alternating Training Final Feed Forward Epoch Test</span></a> and <a class="reference internal" href="homework5.html#successive-validation-test"><span class="std std-ref">Successive Training Final Feed Forward Epoch Test</span></a> that the
Alternating training method (Method 1) resulted in a smaller error after 15 iterations of
training for the first input, but the successive method resulted in a smaller error for the
second input.</p>
<p>Overall, it appears that the alternating method was able to minimize errors across both inputs more evenly than Method 2.
The largest Error for Method 1 was 0.0550343269 which is still less than the largest Error for Method 2, 0.1144057096.
The smallest error was produced by Method 2 (0.0273416978), which is less than the smallest error produced by Method 1.</p>
<p>With just this limited test, it is hard to say if one method is superior to the other. What can be observed in this small experiment is that Method 1
appears to minimize errors across all inputs better than Method 2. Intuitively, Method 1 seems as though it would perform better overall as it is less
likely to be pulled away to a local minimum for one input and then get stuck there even though another, lower minimum is available. The downside to this
is that if the initial weights are such that the training quickly converges to a local minimum, it might end up getting “stuck” in a non-optimal minimum.</p>
<p>Method 2 can overcome such a problem because it trains on one set of input/output pairs at a time. The downside to this approach is that the weights may
minimize the error for one input, but that potentially force the error to be very high for another input. Then, as Method 2 trains on future inputs, it
no longer considers the impact of previous inputs/output pairs. The end result of this can be seen in <a class="reference internal" href="homework5.html#successive-validation-test"><span class="std std-ref">Successive Training Final Feed Forward Epoch Test</span></a> where the smallest error is very small, but the largest error is much larger than the largest from Method 1.</p>
<p><strong>What do you think might happen if we approached the problem in the same way as above, but with different initial values for the weights?</strong></p>
<p>Adjusting the initial weights would affect how far the optimization process would go in the direction of training the Neural Network for the first input.
When looking at the results of the successive method, we see that the error for the first input becomes very small by the 15th iteration (<a class="reference internal" href="homework5.html#successive-training-input-1-15-rounds"><span class="std std-ref">Result of 15 Iterations for Input 1</span></a>).
However, immediately after when it starts training the second input, the error is significant for the new input, so it must undo some of the training done in the previous 15 iterations.
With different initial weights, it is possible that the weights after 15 iterations would be such that the error for the second input could be minimized further than it could with the statically assigned initial values.</p>
<p>By examining the weights and biases of the two final networks (<a class="reference internal" href="homework5.html#successive-training-final"><span class="std std-ref">Method 2 - Final Network</span></a>) (<a class="reference internal" href="homework5.html#alternating-training-final"><span class="std std-ref">Method 1 - Final Network</span></a>), it can be observed that both methods have preserved symmetry of the weights and biases. For some situations, it is possible that the optimal solution may be one with symmetrical weights and biases, however, it is also possible that there may be better solutions which minimize errors across the board with asymmetrical weights and biases for individual perceptrons.</p>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Neural Network</a></h1>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=ankpat&repo=jhu&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="homework5.html">Homework 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="NeuralNetwork.html">NeuralNetwork Package</a></li>
</ul>


        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ankit Patel.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/homework5_analysis.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>